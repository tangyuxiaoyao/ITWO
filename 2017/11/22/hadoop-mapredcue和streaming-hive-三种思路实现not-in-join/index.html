<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>hadoop mapredcue和streaming hive 三种思路实现not in join | ITWO</title><meta name="description" content="hadoop mapredcue和streaming hive 三种思路实现not in join"><meta name="keywords" content="hadoop,streaming,mapreduce,hive,not in"><meta name="author" content="唐钰逍遥"><meta name="copyright" content="唐钰逍遥"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/ITWO/img/favicon.ico"><link rel="stylesheet" href="/ITWO/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://www.tangyuxiaoyao.club/ITWO/2017/11/22/hadoop-mapredcue和streaming-hive-三种思路实现not-in-join/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="hadoop mapredcue和streaming hive 三种思路实现not in join"><meta name="twitter:description" content="hadoop mapredcue和streaming hive 三种思路实现not in join"><meta name="twitter:image" content="https://s2.ax1x.com/2019/07/11/ZRK5z6.jpg"><meta property="og:type" content="article"><meta property="og:title" content="hadoop mapredcue和streaming hive 三种思路实现not in join"><meta property="og:url" content="http://www.tangyuxiaoyao.club/ITWO/2017/11/22/hadoop-mapredcue和streaming-hive-三种思路实现not-in-join/"><meta property="og:site_name" content="ITWO"><meta property="og:description" content="hadoop mapredcue和streaming hive 三种思路实现not in join"><meta property="og:image" content="https://s2.ax1x.com/2019/07/11/ZRK5z6.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="另类的数据结构HashMap" href="http://www.tangyuxiaoyao.club/ITWO/2017/12/06/另类的数据结构HashMap/"><link rel="next" title="spark 本地模式处理文件时报错" href="http://www.tangyuxiaoyao.club/ITWO/2017/11/17/spark-本地模式处理文件时报错/"><script>var GLOBAL_CONFIG = { 
  root: '/ITWO/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"cookieDomain":"http://www.tangyuxiaoyao.club/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Bookmark',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#需求背景"><span class="toc-number">1.</span> <span class="toc-text">需求背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#技术分析"><span class="toc-number">2.</span> <span class="toc-text">技术分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#细节实现"><span class="toc-number">3.</span> <span class="toc-text">细节实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#写在后面"><span class="toc-number">4.</span> <span class="toc-text">写在后面</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://s2.ax1x.com/2019/07/11/ZRnN6S.png)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/ITWO/">ITWO</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad" data-src="https://s2.ax1x.com/2019/07/11/ZRK5z6.jpg" onerror="onerror=null;src='/img/friend_404.gif'"><div class="mobile_author-info__description">毋庸多言,只管前行.</div></div><hr><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a><a class="site-page" href="/ITWO/archives/"><i class="fa-fw fa fa-archive"></i><span> 作品总纲</span></a><a class="site-page" href="/ITWO/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签库</span></a><a class="site-page" href="/ITWO/about/"><i class="fa-fw fa fa-heart"></i><span> 关于自己</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">hadoop mapredcue和streaming hive 三种思路实现not in join</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2017-11-22<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2018-04-16</time></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><blockquote>
<p>公司接入市场流水，有两个路径去存储，有一天突然发现，有个路径下面流水重复，有个路径下面的流水缺失，缺失的路径下面的是生产环境用到的，所以现需要找到这部分缺失的流水，然后打补丁到该hdfs文件路径下。<br><a id="more"></a></p>
<h2 id="技术分析"><a href="#技术分析" class="headerlink" title="技术分析"></a>技术分析</h2><p>首先想到的技术是hive,利用传统的类sql去解决，因为以前有做过类似的需求，left outer join 去解决。<br>第二想到的是hadoop streaming ，其实是领导要求的，用python脚本语言去实现缺失数据打补丁的需求。<br>第三个是原汁原味的hadoop mapreduce经典案例wordcount的改良，其实也是受以前有做过wordcount和使用hadoop mapreduce实现去重联想到的，其实个人觉着这种方法是最简单的。</p>
</blockquote>
<h2 id="细节实现"><a href="#细节实现" class="headerlink" title="细节实现"></a>细节实现</h2><blockquote>
<p>hive<br>思路解析:在hive中一张大表和一张小表去做left ouer join ，<strong>当小表mapping到大表中不存在的关联键时，补值null</strong>,这样我们就可以利用小表id is  null,然后输出a.*,来实现输出大表中有而小表中没有的数据。</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">select</span> b.* <span class="keyword">from</span> bigtable b</div><div class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> littletable l <span class="keyword">on</span> b.id=l.id</div><div class="line"><span class="keyword">where</span> l.id <span class="keyword">is</span> <span class="literal">null</span>;</div></pre></td></tr></table></figure>
<blockquote>
<p>hadoop streaming python<br>思路解析:<strong>streaming 中map输出并且经过shuffle以后得到的reduce输入文件是根据tab（默认的分割符，我们可以使用指定(KeyFieldBasedPartitioner)去达到将其他符号作为分隔符）分割后，同时配合加上-jobconf stream.num.map.output.key.fields=2 参数，使得前两个值作为key做排序后的数据</strong>，所以我们可以利用该特性，初始化一个全局的变量去存储上一条数据的关联键，然后当下一条数据进来的时候去和该值比较如果一样的话，我们将该变量的值重置为空，如果不一样的话，我们输出该条数据（从这里我们可以看出，我们需要保证，两个数据源中的数据我们必须提前确保各自是已经经过去重的。）,还有一个重点是，我们需要保证给小文件中数据打上的标签必须要比大文件中的数据打上的标签值要小，才能保证优先小表中的数据作为参考key去过滤已经存在的数据。<br>ps: Partitioner，默认为HashPartitioner，其根据key的hash值来决定进入哪个partition，每个partition被一个reduce task处理，所以partition的个数等于reduce task的个数</p>
</blockquote>
<p>到reduce端的数据demo<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">1,2,3   0</div><div class="line">1,2,3   1</div><div class="line">312312312       1</div><div class="line">a,b,c   0</div><div class="line">a,b,c   1</div><div class="line">d,e,f   0</div><div class="line">d,e,f   1</div><div class="line">,.,j..  1</div><div class="line">kljl    1</div><div class="line">lkjl    1</div><div class="line">ouroiu  1</div><div class="line">zjljlj  0</div></pre></td></tr></table></figure></p>
<p>start.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"></div><div class="line">hadoop fs -<span class="built_in">test</span> -e <span class="variable">$&#123;3&#125;</span></div><div class="line"><span class="keyword">if</span> [ $? -eq 0 ]; <span class="keyword">then</span></div><div class="line">hadoop fs -rmr <span class="variable">$&#123;3&#125;</span></div><div class="line"><span class="keyword">fi</span></div><div class="line">hadoop jar /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/hadoop-streaming-2.6.0-cdh5.4.0.jar \</div><div class="line">        -D stream.num.map.output.key.fields=2 \</div><div class="line">        -D num.key.fields.for.partition=1 \</div><div class="line">        -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \</div><div class="line">        -input <span class="variable">$&#123;1&#125;</span> <span class="variable">$&#123;2&#125;</span>\</div><div class="line">        -output <span class="variable">$&#123;3&#125;</span> \</div><div class="line">        -file map.py \</div><div class="line">        -file red.py \</div><div class="line">        -mapper <span class="string">"python map.py"</span> \</div><div class="line">        -reducer <span class="string">"python red.py"</span> \</div><div class="line">        -jobconf mapred.reduce.tasks=5 \</div><div class="line">        -jobconf mapred.job.name=<span class="string">"not_in_match"</span></div></pre></td></tr></table></figure>
<p>map.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*-coding:utf-8-*-  </span></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> md5</div><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line">filepath = os.environ[<span class="string">"map_input_file"</span>]</div><div class="line">filename = os.path.split(filepath)[<span class="number">-1</span>]</div><div class="line"></div><div class="line">sep = <span class="string">'\t'</span></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">	detail = line.strip()</div><div class="line">	<span class="keyword">if</span> filename==<span class="string">"littletable"</span>:</div><div class="line">		<span class="keyword">print</span> detail+sep+<span class="string">'0'</span></div><div class="line">	<span class="comment"># big table</span></div><div class="line">	<span class="keyword">else</span>:</div><div class="line">		<span class="keyword">print</span> detail+sep+<span class="string">'1'</span></div></pre></td></tr></table></figure></p>
<p>reduce.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*-coding:utf-8-*-  </span></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"></div><div class="line"></div><div class="line">key_exist = <span class="keyword">None</span></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">	detail = line.strip().split(<span class="string">"\t"</span>)[<span class="number">0</span>]</div><div class="line">	<span class="keyword">if</span>(line.strip().endswith(<span class="string">'\t0'</span>)):</div><div class="line">		key_exist = detail</div><div class="line">	<span class="keyword">else</span>:</div><div class="line">		<span class="keyword">if</span> key_exist = detail:</div><div class="line">			key_exist = <span class="keyword">None</span></div><div class="line">		<span class="keyword">else</span>:</div><div class="line">			<span class="keyword">print</span> detail</div></pre></td></tr></table></figure>
<blockquote>
<p>hadoop mapreduce 原汤化原食<br>思路解析:<strong>在入门hadoop的时候我们都有接触过java版本的wc,其中我们可以看到redcue的输入是（k,vs）</strong>，利用该思路，我们可以改良的下wc，来实现该功能，reduce的输出是（k,sum(vs)）,我们只要保证sum(vs)==1，然后输出该key,就可以实现输出仅在大表中才有的数据。<br>彩蛋:其实利用这种特性，我们就已经可以实现去重了，因为reduce的输入已经是去重后的key然后把对应的map端输出value，拼接为集合放在后面，所以我们只要不输出reduce的value(使用NullWritable来占位)，从而实现数据去重。</p>
</blockquote>
<p>map.class<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Map</span> <span class="keyword">extends</span> <span class="title">MapReduceBase</span> <span class="keyword">implements</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</div><div class="line">    <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text();</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">        String line = value.toString();</div><div class="line">        StringTokenizer tokenizer = <span class="keyword">new</span> StringTokenizer(line);</div><div class="line">        <span class="keyword">while</span> (tokenizer.hasMoreTokens()) &#123;</div><div class="line">            word.set(tokenizer.nextToken());</div><div class="line">            output.collect(word, one);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>reduce.class</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Reduce</span> <span class="keyword">extends</span> <span class="title">MapReduceBase</span> <span class="keyword">implements</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterator&lt;IntWritable&gt; values, OutputCollector&lt;Text, NullWritable&gt; output, Reporter reporter)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</div><div class="line">        <span class="keyword">while</span> (values.hasNext()) &#123;</div><div class="line">            sum += values.next().get();</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">if</span> (sum == one.get()) &#123;</div><div class="line">            output.collect(key, NullWritable.get());</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<blockquote>
<p>提示:在原来版本wc中我们有看到还有使用到combiner（map端的reduce）,但是使用combiner的前提是必须要保证和map的输出指定的数据类型必须是一样的，但是明显我们这里不满足，因为combinner使用的是reduce中的逻辑已经改变了输出数据的数据类型，所以要去掉或者注释掉这部分的代码。</p>
</blockquote>
<h2 id="写在后面"><a href="#写在后面" class="headerlink" title="写在后面"></a>写在后面</h2><blockquote>
<p>我们处理大数据的大前提一定是要保证门清儿<strong>各种框架的优势</strong>，然后再在此基础上使用逻辑规约数据输出我们所需要的数据．</p>
</blockquote>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">唐钰逍遥</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://www.tangyuxiaoyao.club/ITWO/2017/11/22/hadoop-mapredcue和streaming-hive-三种思路实现not-in-join/">http://www.tangyuxiaoyao.club/ITWO/2017/11/22/hadoop-mapredcue和streaming-hive-三种思路实现not-in-join/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/ITWO/tags/hadoop/">hadoop    </a><a class="post-meta__tags" href="/ITWO/tags/streaming/">streaming    </a><a class="post-meta__tags" href="/ITWO/tags/mapreduce/">mapreduce    </a><a class="post-meta__tags" href="/ITWO/tags/hive/">hive    </a><a class="post-meta__tags" href="/ITWO/tags/not-in/">not in    </a></div><div class="post_share"><div class="social-share" data-image="https://s2.ax1x.com/2019/07/11/ZRK5z6.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-buttom"><i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lozad post-qr-code__img" data-src="https://s2.ax1x.com/2019/07/11/ZRQPc6.png"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lozad post-qr-code__img" data-src="https://s2.ax1x.com/2019/07/11/ZRQC1x.png"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-left"><a href="/ITWO/2017/12/06/另类的数据结构HashMap/"><img class="prev_cover lozad" data-src="https://s2.ax1x.com/2019/07/11/ZR1SQx.gif" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Previous Post</div><div class="prev_info"><span>另类的数据结构HashMap</span></div></a></div><div class="next-post pull-right"><a href="/ITWO/2017/11/17/spark-本地模式处理文件时报错/"><img class="next_cover lozad" data-src="https://s2.ax1x.com/2019/07/11/ZR1SQx.gif" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>spark 本地模式处理文件时报错</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2018/04/11/如何在hadoop-streaming-mapreduce中-使用python第三方包/" title="如何在hadoop streaming mapreduce中 使用python第三方包"><img class="relatedPosts_cover lozad" data-src="https://s2.ax1x.com/2019/07/11/ZR1SQx.gif"><div class="relatedPosts_title">如何在hadoop streaming mapreduce中 使用python第三方包</div></a></div><div class="relatedPosts_item"><a href="/2018/01/11/hadoop-streaming-使用中遇到的问题总结/" title="hadoop streaming 使用中遇到的问题总结"><img class="relatedPosts_cover lozad" data-src="https://s2.ax1x.com/2019/07/11/ZR1SQx.gif"><div class="relatedPosts_title">hadoop streaming 使用中遇到的问题总结</div></a></div><div class="relatedPosts_item"><a href="/2017/11/02/hadoop-二次排序join的实现/" title="hadoop  streaming 二次排序join的实现"><img class="relatedPosts_cover lozad" data-src="https://s2.ax1x.com/2019/07/11/ZR1SQx.gif"><div class="relatedPosts_title">hadoop  streaming 二次排序join的实现</div></a></div><div class="relatedPosts_item"><a href="/2017/12/22/yarn-详解/" title="yarn 详解"><img class="relatedPosts_cover lozad" data-src="https://s2.ax1x.com/2019/07/11/ZR1SQx.gif"><div class="relatedPosts_title">yarn 详解</div></a></div><div class="relatedPosts_item"><a href="/2018/07/18/使用hive-mapreduce给大数据全局排序，同时巧用该方法实现hbase的预分区/" title="使用hive/mapreduce给大数据全局排序，同时巧用该方法实现hbase的预分区"><img class="relatedPosts_cover lozad" data-src="https://s2.ax1x.com/2019/07/11/ZR1SQx.gif"><div class="relatedPosts_title">使用hive/mapreduce给大数据全局排序，同时巧用该方法实现hbase的预分区</div></a></div><div class="relatedPosts_item"><a href="/2018/01/10/给hive-mapreduce分配队列和优先级/" title="给hive mapreduce分配队列和优先级"><img class="relatedPosts_cover lozad" data-src="https://s2.ax1x.com/2019/07/11/ZR1SQx.gif"><div class="relatedPosts_title">给hive mapreduce分配队列和优先级</div></a></div></div><div class="clear_both"></div></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By 唐钰逍遥</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="http://www.tangyuxiaoyao.club/">ITWO</a>!</div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="Read Mode"> </i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion">簡</a><i class="fa fa-moon-o nightshift" id="nightshift" title="Dark Mode"></i></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script src="/ITWO/js/utils.js"></script><script src="/ITWO/js/main.js"></script><script src="/ITWO/js/nightshift.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script src="/ITWO/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="/ITWO/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/ITWO/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":100,"height":200},"mobile":{"show":false},"log":false});</script></body></html>