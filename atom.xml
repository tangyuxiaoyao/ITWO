<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ITWO</title>
  
  <subtitle>毋庸多言,只管前行.</subtitle>
  <link href="/ITWO/atom.xml" rel="self"/>
  
  <link href="https://www.tangyuxiaoyao.club/ITWO/"/>
  <updated>2018-07-18T06:00:34.630Z</updated>
  <id>https://www.tangyuxiaoyao.club/ITWO/</id>
  
  <author>
    <name>唐钰逍遥</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用hive/mapreduce给大数据全局排序，同时巧用该方法实现hbase的预分区</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/07/18/%E4%BD%BF%E7%94%A8hive-mapreduce%E7%BB%99%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A8%E5%B1%80%E6%8E%92%E5%BA%8F%EF%BC%8C%E5%90%8C%E6%97%B6%E5%B7%A7%E7%94%A8%E8%AF%A5%E6%96%B9%E6%B3%95%E5%AE%9E%E7%8E%B0hbase%E7%9A%84%E9%A2%84%E5%88%86%E5%8C%BA/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/07/18/使用hive-mapreduce给大数据全局排序，同时巧用该方法实现hbase的预分区/</id>
    <published>2018-07-18T01:50:44.000Z</published>
    <updated>2018-07-18T06:00:34.630Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><blockquote><p>完成排序，打行号，然后根据分位数找到分界点。</p></blockquote><a id="more"></a><h2 id="hive篇"><a href="#hive篇" class="headerlink" title="hive篇"></a>hive篇</h2><h3 id="造测试数据"><a href="#造测试数据" class="headerlink" title="造测试数据"></a>造测试数据</h3><blockquote><p>直接上代码，如下的代码可以仿造出64位的加密字符串，达到模拟某种加密的方式，如此方式造了100w的数据用于测试。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">key = <span class="string">""</span>.join(random.choice(<span class="string">"0123456789ABCDEF"</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">64</span>))</div></pre></td></tr></table></figure><h3 id="导数据"><a href="#导数据" class="headerlink" title="导数据"></a>导数据</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">use</span> koulb;</div><div class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> t_card_info (<span class="keyword">key</span> <span class="keyword">string</span>)</div><div class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></div><div class="line"><span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span></div><div class="line">Location <span class="string">'/user/koulingbo/cardInfo'</span>;</div></pre></td></tr></table></figure><h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_card_info_row <span class="keyword">as</span> </div><div class="line"><span class="keyword">select</span> row_number() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">key</span>) <span class="keyword">as</span> rn,<span class="keyword">key</span> <span class="keyword">from</span> t_card_info;</div><div class="line"></div><div class="line"> number of reducers: 1</div></pre></td></tr></table></figure><blockquote><p>众所周知 hive 中的order by 适用于全局排序，所以它只能是给到一个reduce来完成，因为不同的parttion来分区到不同的reduce就决定了只能reduce内部有序，如果你想达到全局排序只能够是一个reduce。</p></blockquote><h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><blockquote><p>思路：使用分位数函数来定位切割点，当然最初你要根据tsv 的大小、snappy压缩比、以及region的大小（根据hfile以及其个数确定）来确定要分成几个region,本文暂定为100个分区来做演示。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">var="1/100"</div><div class="line">for i in &#123;2..99&#125;</div><div class="line">do</div><div class="line">        var="$var,$i/100"</div><div class="line">done</div><div class="line">echo $var</div><div class="line"></div><div class="line"></div><div class="line">hive -S -e "use koulb;create table t_card_info_res as select k.key from \</div><div class="line">(select explode(percentile_approx(rn,array($&#123;var&#125;),1000000))as keyRange from t_card_info_row) r"</div><div class="line">join t_card_info_row k on r.keyRange=k.rn;</div></pre></td></tr></table></figure><h2 id="mapreduce部分"><a href="#mapreduce部分" class="headerlink" title="mapreduce部分"></a>mapreduce部分</h2><p>==未完待续==</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;完成排序，打行号，然后根据分位数找到分界点。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/python/"/>
    
      <category term="mapreduce" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/mapreduce/"/>
    
      <category term="hive" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hive/"/>
    
      <category term="row_number" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/row-number/"/>
    
      <category term="percentile" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/percentile/"/>
    
      <category term="explode" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/explode/"/>
    
  </entry>
  
  <entry>
    <title>mapreduce 温故</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/07/06/mapredcue%20%E6%B8%A9%E4%B9%A0/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/07/06/mapredcue 温习/</id>
    <published>2018-07-06T02:34:45.771Z</published>
    <updated>2018-07-06T02:34:45.771Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求背景："><a href="#需求背景：" class="headerlink" title="需求背景："></a>需求背景：</h2><blockquote><p>做大数据有一段时间了，梳理下用到mapreduce的一些问题和解决方案。<a id="more"></a></p></blockquote><h2 id="mapreduce"><a href="#mapreduce" class="headerlink" title="mapreduce"></a>mapreduce</h2><blockquote><p>mapreduce:顾名思义，map做映射，reduce做规约。<br>主要分以下步骤：<br>1.输入分块<br>2.map<br>3.shuffer<br>4.reduce</p></blockquote><p><img src="/ITWO/assets/mapreduce01.jpg" alt="mapredcue流程图"><br>重点是shuffer阶段</p><h2 id="reduce个数的计算方法"><a href="#reduce个数的计算方法" class="headerlink" title="reduce个数的计算方法:"></a>reduce个数的计算方法:</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">double</span> bytes = Math.max(totalInputFileSize, bytesPerReducer);</div><div class="line"><span class="keyword">int</span> reducers = (<span class="keyword">int</span>) Math.ceil(bytes / bytesPerReducer);</div><div class="line">reducers = Math.max(<span class="number">1</span>, reducers);</div><div class="line">reducers = Math.min(maxReducers, reducers);</div></pre></td></tr></table></figure><blockquote><p>　从计算逻辑可以看出该量由输入文件的大小以及设置的每个reduce可以处理的字节数大小决定．</p></blockquote><p><img src="/ITWO/assets/mapreduce02.png" alt="shuffer流程图"></p><h2 id="流程细节："><a href="#流程细节：" class="headerlink" title="流程细节："></a>流程细节：</h2><h3 id="map输出过程："><a href="#map输出过程：" class="headerlink" title="map输出过程："></a>map输出过程：</h3><blockquote><p>&ensp;&ensp;&ensp;&ensp;如果没有reduce阶段，则直接输出到hdfs上，如果有reduce作业，则每个map方法的输出在写磁盘前先在内存中缓存。每个map task都有一个环状的内存缓冲区，存储着map的输出结果，默认100m，在写磁盘时，根据reduce的数量把数据划分为相应的分区(使用默认的分区算法（对输入文件的kv中对key hash后再对reduce task数量取模(reduce个数的算法见前文)),默认的hashPartioner只会作用默认分隔符分割以后的key，如果需要自定义分区，则需要你自定义二次分区比如keyfieldParttioner来实现,在每个分区中数据进行内排序，分区的个数和reduce的个数是一致的，在每次当缓冲区快满的时候由一个独立的线程将缓冲区的数据以一个溢出文件的方式存放到磁盘(这个溢写是由单独线程来完成，不影响往缓冲区写map结果的线程。溢写线程启动时不应该阻止map的结果输出，所以整个缓冲区有个溢写的比例spill.percent。这个比例默认是0.8，也就是当缓冲区的数据已经达到阈值（buffer size <em> spill percent = 100MB </em> 0.8 = 80MB），溢写线程启动，锁定这80MB的内存，执行溢写过程。Map task的输出结果还可以往剩下的20MB内存中写，互不影响。)，当整个map task结束后再对磁盘中这个map task产生的所有溢出文件做合并，被合并成已分区且已排序的输出文件。然后reduce开始fetch（拉取）map端合并好对应分区的数据，然后在reduce端合并（因为会有很多map的输出，需要合并），此时在reduce端也会进行一次sort,确保所有map的输出都排序并合并成完成以后，才会启动reduce task,所以怎么才能确保你在reduce逻辑处理时拿到的是你要的排序后的数据配合你的处理就至关重要了。</p></blockquote><h3 id="reducer如何知道要从哪个tasktracker取得map输出呢？"><a href="#reducer如何知道要从哪个tasktracker取得map输出呢？" class="headerlink" title="reducer如何知道要从哪个tasktracker取得map输出呢？"></a>reducer如何知道要从哪个tasktracker取得map输出呢？</h3><blockquote><p>&ensp;&ensp;&ensp;&ensp;map任务成功完成以后，他们会通知其父tasktracker状态已更新，然后taskTracker进而通知jobTracker。这些通知在前面的心跳机制中传输。因此，对于指定作业，jobTracker知道map输出和taskTracker之间的映射关系。reducer中的一个线程定期询问jobTracher以便获取map输出的位置,直到它获得所有输出位置。</p></blockquote><h3 id="map和reduce如何合理控制自己的个数？"><a href="#map和reduce如何合理控制自己的个数？" class="headerlink" title="map和reduce如何合理控制自己的个数？"></a>map和reduce如何合理控制自己的个数？</h3><blockquote><p>&ensp;&ensp;&ensp;&ensp;map的个数是由dfs.block.size控制，该配置可以在执行程序之前由参数（见下文）控制，默认配置位于hdfs-site.xml中dfs.block.size控制，1.x的默认配置为64m,2.x的默认配置为128m,</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"> <span class="keyword">long</span> goalSize = totalSize / (numSplits == <span class="number">0</span> ? <span class="number">1</span> : numSplits);</div><div class="line"> <span class="keyword">long</span> minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.</div><div class="line">    FileInputFormat.SPLIT_MINSIZE, <span class="number">1</span>), minSplitSize);</div><div class="line"><span class="keyword">long</span> blockSize = file.getBlockSize();</div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">long</span> <span class="title">computeSplitSize</span><span class="params">(<span class="keyword">long</span> goalSize, <span class="keyword">long</span> minSize,</span></span></div><div class="line"><span class="function"><span class="params">                                     <span class="keyword">long</span> blockSize)</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> Math.max(minSize, Math.min(goalSize, blockSize));</div><div class="line">&#125;</div></pre></td></tr></table></figure><blockquote><p>&ensp;&ensp;&ensp;&ensp;从上面可以看出，最终的split size是由三个因素决定，goalsize为map输入数据除以用户自己设置的map个数（默认为1）得到的;minsize为mapred-site.xml配置的mapred.min.split.size决定，因为minSplitSize为1;第三个影响因素为blocksize,这个看配置，最终我们可以得出,如果不设置min.size,则由blocksize决定，如果设置了，则是由这两者中大的一个决定。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="built_in">set</span> mapred.min.split.size=256000000;        -- 决定每个map处理的最大的文件大小，单位为B</div><div class="line"></div><div class="line">方法1</div><div class="line"><span class="built_in">set</span> mapred.reduce.tasks=10;  -- 设置reduce的数量</div><div class="line">方法2</div><div class="line"><span class="built_in">set</span> hive.exec.reducers.bytes.per.reducer=1073741824 -- 每个reduce处理的数据量,默认1GB</div></pre></td></tr></table></figure><p>block_size : hdfs的文件块大小，默认为64M，可以通过参数dfs.block.size设置<br>total_size : 输入文件整体的大小<br>input_file_num : 输入文件的个数</p><p>（1）默认map个数</p><blockquote><p>如果不进行任何设置，默认的map个数是和blcok_size相关的。<br>   default_num = total_size / block_size;</p></blockquote><p>（2）期望大小</p><blockquote><p>可以通过参数mapred.map.tasks来设置程序员期望的map个数，但是这个个数只有在大于default_num的时候，才会生效。<br>   goal_num = mapred.map.tasks;</p></blockquote><p>（3）设置处理的文件大小</p><blockquote><p>可以通过mapred.min.split.size 设置每个task处理的文件大小，但是这个大小只有在大于block_size的时候才会生效。<br>   split_size = max(mapred.min.split.size, block_size);<br>   split_num = total_size / split_size;</p></blockquote><p>（4）计算的map个数</p><blockquote><p>compute_map_num = min(split_num,  max(default_num, goal_num))</p><p>&ensp;&ensp;&ensp;&ensp;除了这些配置以外，mapreduce还要遵循一些原则。 mapreduce的每一个map处理的数据是不能跨越文件的，也就是说min_map_num &gt;= input_file_num。 所以，最终的map个数应该为：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">final_map_num = max(compute_map_num, input_file_num)</div></pre></td></tr></table></figure><blockquote><p>经过以上的分析，在设置map个数的时候，可以简单的总结为以下几点：<br>（1）如果想增加map个数，则设置mapred.map.tasks 为一个较大的值。<br>（2）如果想减小map个数，则设置mapred.min.split.size 为一个较大的值。</p></blockquote><p>reduce个数的设置则相对简单，要么你设置mapred.reduce.tasks的数值，要么你在hive中可以设置每个reduce可以处理的字节数，从而约束reduce的个数。</p><blockquote><p>小技巧</p></blockquote><p>&ensp;&ensp;&ensp;&ensp;在hive中带空的设置参数可以打印出当前该参数的设置值。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hive&gt; set dfs.block.size;</div><div class="line">dfs.block.size=268435456</div><div class="line">hive&gt; set mapred.map.tasks;</div><div class="line">mapred.map.tasks=2</div></pre></td></tr></table></figure><h2 id="mapreduce进度说明"><a href="#mapreduce进度说明" class="headerlink" title="mapreduce进度说明"></a>mapreduce进度说明</h2><h3 id="1-Prepare"><a href="#1-Prepare" class="headerlink" title="1. Prepare"></a>1. Prepare</h3><blockquote><p>准备数据，抓取Map过来的输出（进度：0~33%）</p></blockquote><h3 id="2-Sort"><a href="#2-Sort" class="headerlink" title="2. Sort"></a>2. Sort</h3><blockquote><p>排序阶段（进度：33%~66%）</p></blockquote><h3 id="3-Reduce"><a href="#3-Reduce" class="headerlink" title="3. Reduce"></a>3. Reduce</h3><blockquote><p>真正的reduce计算阶段，执行你所写的reduce代码（进度：66%~100%）.<br>如果前面66%速度很快，后面慢的话就是reduce部分没有写好；否则才是数据量大的问题。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求背景：&quot;&gt;&lt;a href=&quot;#需求背景：&quot; class=&quot;headerlink&quot; title=&quot;需求背景：&quot;&gt;&lt;/a&gt;需求背景：&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;做大数据有一段时间了，梳理下用到mapreduce的一些问题和解决方案。
    
    </summary>
    
    
      <category term="mapreduce" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/mapreduce/"/>
    
      <category term="shuffer" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/shuffer/"/>
    
  </entry>
  
  <entry>
    <title>hbase/es load 问题总结</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/07/06/hbase-load-%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/07/06/hbase-load-问题总结/</id>
    <published>2018-07-06T01:48:49.000Z</published>
    <updated>2018-07-11T09:54:50.301Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><blockquote><p>公司指标2.0开发了很多新的指标，数据量很客观，需要导入到列式存储数据库中，实现毫秒级别的响应查询，之前也有一版本的导入是导入到hbase中，但是hbase对于高并发的响应不是很理想，但凡高并发上来以后，时效性就会有影响。</p></blockquote><a id="more"></a><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><h3 id="HBase部分"><a href="#HBase部分" class="headerlink" title="HBase部分"></a>HBase部分</h3><blockquote><p>因为数据量大的缘故，使用的是将多个HDFS文件下的数据切割然后组合成tsv，再使用PUT元素转换为HFile文件，进而使用bulkload导入hbase中。</p></blockquote><h3 id="ES部分"><a href="#ES部分" class="headerlink" title="ES部分"></a>ES部分</h3><blockquote><p>复用之前生成好的tsv文件，组装好MapWritable，然后使用EsOutputFormat完成格式化输出。</p></blockquote><h2 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h2><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><blockquote><p>这部分时间耗费在本地调试过渡到集群跑批，因为在本地集群单节点测试，你再怎么折腾也不出现配置文件找不到的问题，当然这里的前提是我使用了Properties文件做为传递配置参数的载体，在多节点测试服务器跑批的时候就会出现找不到配置文件的问题，因为该配置文件只会存在client端，而不会分发到各个其他的节点，所以会有找不到配置文件的问题出现。当然这部分也因为在使用的是ToolRunner.run(conf, new BootApplication(), args)，所以考虑过使用-files#filelink,具体用法参考字典文件分发的那篇文章，但是使用的时候，并没有达到达到预期效果，打开方式不对？<br>后来的做法是先用Properties对象解析外部的绝对路径的配置问价，将拿到的配置放到一直在框架中的上下文对象Context中的Configuration中，这样只要在client端提交任务的地方，传值给它，那么无论在框架运行的什么阶段都可以读到外部配置文件中的配置项。</p></blockquote><h3 id="HA的开启"><a href="#HA的开启" class="headerlink" title="HA的开启"></a>HA的开启</h3><blockquote><p>在本地的测试过程中，因为本地没有开启NM的HA，所以只需要配置如下内容就可以完成程序和集群的对接：</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">fs.defaultFS=hdfs://hostname:8020</div></pre></td></tr></table></figure><blockquote><p>当然如果没有配置，也可以做本地文件的测试。</p><p>要对接开启了NM的HA的集群除了以上的配置还需要配置如下的内容:</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">fs.nameservices=nameservice</div><div class="line"><span class="meta">#</span><span class="bash">指定nameservice服务下有几个namenode</span></div><div class="line">fs.ha.namenodes.nameservice=namenode33,namenode134</div><div class="line"><span class="meta">#</span><span class="bash">指定各个namenode节点的访问链接</span></div><div class="line">fs.namenode.rpc-address.nameservice.namenode33=node-01.upsmart.com:8020</div><div class="line">fs.namenode.rpc-address.nameservice.namenode134=node-02.upsmart.com:8020</div><div class="line">fs.client.failover.proxy.provider.nameservice=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</div></pre></td></tr></table></figure><h3 id="habse-依赖问题"><a href="#habse-依赖问题" class="headerlink" title="habse 依赖问题"></a>habse 依赖问题</h3><blockquote><p>在导入数据到hbase的过程中在一集群导入没有问题，原本以为已经可以收工，但是后来在开启的HA的另一集群测试，出现了少包的问题，这部分可能跟集群安装依赖包的寡众有关系，但是保不齐那个集群就少包了，所以你要做的是把所有的能用到的包都打到jar包里，但是打包的时候又遇到如下的问题：<br>在打包的过程中有个包一直打不进去后来 </p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mvn -X clean install &gt;log</div></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-hadoop-compat<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0.0-cdh5.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div></pre></td></tr></table></figure><blockquote><p>定位到如下的ERROR</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="string">'dependencies.dependency.version'</span> <span class="keyword">for</span> commons-logging:commons-logging:jar is missing. @</div></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></div><div class="line">  <span class="comment">&lt;!-- General dependencies --&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>commons-logging<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-logging<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.commons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-math<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></div></pre></td></tr></table></figure><blockquote><p>原本以为去掉这些依赖就可以万事大吉，但是并没有奏效，后来使用了最彻底的办法依照大版本相同应该改动不大的原则换了版本到1.1.10,然后打包后问题解决。<br>以下方法并没有解决问题：</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>commons-logging<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-logging<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></div></pre></td></tr></table></figure><blockquote><p>我的理解是解析pom的时候和打包去除某些依赖是不同的解析顺序</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">    &lt;groupId&gt;org.htrace&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;htrace-core&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;3.0.4&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div><div class="line"></div><div class="line">&lt;dependency&gt;</div><div class="line">    &lt;groupId&gt;org.apache.htrace&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;htrace-core&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;3.1.0-incubating&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure><blockquote><p>这两个包都要加否则会报错找不到。</p></blockquote><h3 id="mapreduce-过程报错"><a href="#mapreduce-过程报错" class="headerlink" title="mapreduce 过程报错"></a>mapreduce 过程报错</h3><blockquote><p>beyond physical memory limits</p></blockquote><h4 id="map-reduce-阶段报错"><a href="#map-reduce-阶段报错" class="headerlink" title="map reduce  阶段报错"></a>map reduce  阶段报错</h4><blockquote><p>解决方案：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yarn.scheduler.minimum-allocation-mb 调节大点</div></pre></td></tr></table></figure></p></blockquote><h4 id="如果是只有reduce阶段报错"><a href="#如果是只有reduce阶段报错" class="headerlink" title="如果是只有reduce阶段报错"></a>如果是只有reduce阶段报错</h4><blockquote><p>解决方案：可以通过增大reduce的个数来分散reduce端的处理压力</p></blockquote><h4 id="reduce-100-beyond-physical-memory-limits"><a href="#reduce-100-beyond-physical-memory-limits" class="headerlink" title="reduce 100%  beyond physical memory limits"></a>reduce 100%  beyond physical memory limits</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#9</div><div class="line">        at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:<span class="number">134</span>)</div><div class="line">        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:<span class="number">376</span>)</div><div class="line">        at org.apache.hadoop.mapred.YarnChild$<span class="number">2</span>.run(YarnChild.java:<span class="number">164</span>)</div><div class="line">        at java.security.AccessController.doPrivileged(Native Method)</div><div class="line">        at javax.security.auth.Subject.doAs(Subject.java:<span class="number">422</span>)</div><div class="line">        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:<span class="number">1920</span>)</div><div class="line">        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:<span class="number">158</span>)</div><div class="line">Caused by: java.lang.OutOfMemoryError: Java heap space</div></pre></td></tr></table></figure><blockquote><p>解决方案：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">-D mapreduce.reduce.shuffle.memory.limit.percent=0.1</div></pre></td></tr></table></figure></p></blockquote><h3 id="ClassNotFoundException"><a href="#ClassNotFoundException" class="headerlink" title="ClassNotFoundException"></a>ClassNotFoundException</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">java.lang.ClassNotFoundException: Class org.elasticsearch.hadoop.mr.EsOutputFormat not found</div></pre></td></tr></table></figure><blockquote><p>解决方案：</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Create job</span></div><div class="line">job.setJarByClass(BootApplication.class);</div></pre></td></tr></table></figure><blockquote><p>问题分析：看代码，一目了然。</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setJarByClass</span><span class="params">(Class cls)</span> </span>&#123;</div><div class="line">    String jar = findContainingJar(cls);</div><div class="line">    <span class="keyword">if</span> (jar != <span class="keyword">null</span>) &#123;</div><div class="line">        <span class="keyword">this</span>.setJar(jar);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="es导入"><a href="#es导入" class="headerlink" title="es导入"></a>es导入</h3><h4 id="Limit-of-total-fields-1000-in-index-card-nature-has-been-exceeded"><a href="#Limit-of-total-fields-1000-in-index-card-nature-has-been-exceeded" class="headerlink" title="Limit of total fields [1000] in index [card_nature] has been exceeded"></a>Limit of total fields [1000] in index [card_nature] has been exceeded</h4><blockquote><p>解决方案：</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">curl  -H "Content-Type: application/json"  -XPUT http://192.168.88.126:9200/card_nature/_settings?pretty=true -d '&#123;"settings": </div><div class="line">&#123;"index.mapping.total_fields.limit": 100000&#125;&#125;'</div></pre></td></tr></table></figure><h4 id="建立索引的时候报错：-usr-bin-curl-Argument-list-too-long"><a href="#建立索引的时候报错：-usr-bin-curl-Argument-list-too-long" class="headerlink" title="建立索引的时候报错：/usr/bin/curl: Argument list too long"></a>建立索引的时候报错：/usr/bin/curl: Argument list too long</h4><blockquote><p>解决方案<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">curl -H 'content-type: application/json' -XPUT \</div><div class="line">  -d @- 'http://localhost:9200/card_nature' &lt;&lt;CURL_DATA</div><div class="line">&#123;</div><div class="line">    "settings": &#123;</div><div class="line">                "number_of_replicas": 0,</div><div class="line">                "number_of_shards": 6,</div><div class="line">                "refresh_interval": "-1",</div><div class="line">                "index.mapping.total_fields.limit": 100000</div><div class="line">    &#125;,</div><div class="line">        "mappings": &#123;</div><div class="line">            "card_nature": &#123;</div><div class="line">                'properties': &#123;</div><div class="line">                    "CP0124":&#123;"type": "date","format": "yyyy-mm", "index": "false"&#125;,</div><div class="line">                    "CP0125":&#123;"type": "date","format": "yyyy-mm", "index": "false"&#125;,</div><div class="line">                    "CP0126":&#123;"type": "keyword", "index": "false"&#125;,</div><div class="line">                    "CP0127":&#123;"type": "keyword", "index": "false"&#125;,</div><div class="line">                    "CP0128":&#123;"type": "keyword", "index": "false"&#125;,</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">&#125;</div><div class="line">CURL_DATA</div><div class="line"></div><div class="line">curl -H "Content-Type: application/json"  -XPUT http://192.168.88.126:9200/card_nature/_settings?pretty=true -d '&#123;"settings": &#123;"refresh_interval": "5s","number_of_replicas":1&#125;&#125;'</div><div class="line">curl -XPOST  http://192.168.88.126:9200/card_nature/_refresh</div></pre></td></tr></table></figure></p></blockquote><h4 id="建索引的时候报错"><a href="#建索引的时候报错" class="headerlink" title="建索引的时候报错"></a>建索引的时候报错</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">"type": "mapper_parsing_exception",</div><div class="line">"reason": "No handler for type [string] declared on field [CP0125]"</div></pre></td></tr></table></figure><blockquote><p>解决方案:5.x以上已经没有string类型。如果需要分词的话使用text，不需要分词使用keyword。</p></blockquote><h3 id="es建立索引总结"><a href="#es建立索引总结" class="headerlink" title="es建立索引总结"></a>es建立索引总结</h3><blockquote><p>建立索引的时候关闭索引更新，别设置副本，如果字段超过1000，您就按照上面那样设置。<br>数据导入完成以后要么您开启五秒间隔更新，为了防止以后小批量的数据导入。<br>要么您每次导入数据之后手动使用最下面手动更新索引。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求背景&quot;&gt;&lt;a href=&quot;#需求背景&quot; class=&quot;headerlink&quot; title=&quot;需求背景&quot;&gt;&lt;/a&gt;需求背景&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;公司指标2.0开发了很多新的指标，数据量很客观，需要导入到列式存储数据库中，实现毫秒级别的响应查询，之前也有一版本的导入是导入到hbase中，但是hbase对于高并发的响应不是很理想，但凡高并发上来以后，时效性就会有影响。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="hbase" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hbase/"/>
    
      <category term="bulkload" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/bulkload/"/>
    
      <category term="es" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/es/"/>
    
  </entry>
  
  <entry>
    <title>比较两个文件内容的不同</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/06/08/%E6%AF%94%E8%BE%83%E4%B8%A4%E4%B8%AA%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E7%9A%84%E4%B8%8D%E5%90%8C/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/06/08/比较两个文件内容的不同/</id>
    <published>2018-06-08T02:29:06.000Z</published>
    <updated>2018-06-08T02:32:12.410Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景描述"><a href="#背景描述" class="headerlink" title="背景描述"></a>背景描述</h2><blockquote><p>比较两个文件的不同</p></blockquote><a id="more"></a><h2 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">koulb@koulb-ubantu:~/testdata$ more a</div><div class="line">1</div><div class="line">a</div><div class="line">b</div><div class="line">d</div><div class="line">c</div><div class="line">koulb@koulb-ubantu:~/testdata$ more b</div><div class="line">a</div><div class="line">b</div></pre></td></tr></table></figure><blockquote><p>这时你去比较</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">koulb@koulb-ubantu:~/testdata$ comm -23 a b</div><div class="line">1</div><div class="line">f</div><div class="line">comm: 文件1 没有被正确排序</div><div class="line">e</div><div class="line">a</div><div class="line">b</div><div class="line">c</div><div class="line">d</div></pre></td></tr></table></figure><blockquote><p>你需要做的是:</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">koulb@koulb-ubantu:~/testdata$ sort a &gt;c</div><div class="line">koulb@koulb-ubantu:~/testdata$ comm -23 c b</div><div class="line">1</div><div class="line">c</div><div class="line">d</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景描述&quot;&gt;&lt;a href=&quot;#背景描述&quot; class=&quot;headerlink&quot; title=&quot;背景描述&quot;&gt;&lt;/a&gt;背景描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;比较两个文件的不同&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="shell" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/shell/"/>
    
      <category term="file" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/file/"/>
    
      <category term="comm" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/comm/"/>
    
  </entry>
  
  <entry>
    <title>安装es遇到的问题和解决方案</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/05/28/%E5%AE%89%E8%A3%85es%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/05/28/安装es遇到的问题和解决方案/</id>
    <published>2018-05-28T01:03:14.000Z</published>
    <updated>2018-05-28T01:24:00.749Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><blockquote><p>在安装es过程中遇到的问题总结。</p></blockquote><a id="more"></a><h2 id="问题一"><a href="#问题一" class="headerlink" title="问题一"></a>问题一</h2><blockquote><ol><li>max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]</li></ol></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo vi /etc/sysctl.conf</div><div class="line">在最后一行加入如下配置</div><div class="line">vm.max_map_count=262144</div></pre></td></tr></table></figure><hr><h2 id="问题二"><a href="#问题二" class="headerlink" title="问题二"></a>问题二</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bootstrap.memory_lock: true</div></pre></td></tr></table></figure><blockquote><p>开起如上配置启动报错如下</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="number">2</span>. bootstrap checks failed  memory locking requested <span class="keyword">for</span> elasticsearch process but memory is not locked</div><div class="line"></div><div class="line">These can be adjusted by modifying /etc/security/limits.conf, <span class="keyword">for</span> example:</div><div class="line">#allow user 'upsmart' mlockall</div><div class="line">upsmart soft memlock unlimited</div><div class="line">upsmart hard memlock unlimited</div><div class="line">If you are logged in interactively, you will have to re-login <span class="keyword">for</span> the <span class="keyword">new</span> limits to take effect.</div><div class="line">These can be adjusted by modifying /etc/security/limits.conf, <span class="keyword">for</span> example:</div><div class="line">#allow user 'upsmart' mlockall</div><div class="line">upsmart soft memlock unlimited</div><div class="line">upsmart hard memlock unlimited</div><div class="line">If you are logged in interactively, you will have to re-login <span class="keyword">for</span> the <span class="keyword">new</span> limits to take effect.</div></pre></td></tr></table></figure><blockquote><p>根据以上提示要修改以下内容</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">sudo vim /etc/security/limits.conf</div><div class="line"><span class="meta">#</span><span class="bash">allow user <span class="string">'upsmart'</span> mlockall</span></div><div class="line">upsmart soft memlock unlimited</div><div class="line">upsmart hard memlock unlimited</div></pre></td></tr></table></figure><hr><h2 id="写在后面"><a href="#写在后面" class="headerlink" title="写在后面"></a>写在后面</h2><blockquote><p>改的过程中发现启动程序不起作用，后来发现提示下面还有一句话提示的是如果不起作用，需要重新建立登录，后来关闭该会话，然后重启程序。（细节很重要！！！）</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求背景&quot;&gt;&lt;a href=&quot;#需求背景&quot; class=&quot;headerlink&quot; title=&quot;需求背景&quot;&gt;&lt;/a&gt;需求背景&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;在安装es过程中遇到的问题总结。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="es" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/es/"/>
    
  </entry>
  
  <entry>
    <title>mongo查询慢排查</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/05/23/mongo%E6%9F%A5%E8%AF%A2%E6%85%A2%E6%8E%92%E6%9F%A5/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/05/23/mongo查询慢排查/</id>
    <published>2018-05-23T06:11:34.000Z</published>
    <updated>2018-05-23T07:30:24.257Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><blockquote><p>线上建模结果缓存在mongo中，之前由于数据量较少，进来一段时间由于老系统业务迁移过来，数据量也上来了，随之而来暴露出来一个问题.</p></blockquote><a id="more"></a><blockquote><p>查询很慢，但是只是个别客户的账号有问题，起初以为是客户建模的数据（客户部分代码有异常）有问题导致，但是从日志来看也有正常返回的数据但是也会有超时，后来把该客户的数据导入到开发集群来排查问题。</p></blockquote><h2 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h2><blockquote><p>之前接口层面建的索引执行计划给出执行过程</p></blockquote><p><img src="/ITWO/assets/index_01.png" alt="之前接口层面建的索引执行计划给出执行过程"></p><blockquote><p>分解图示：<br>Explain部分分为</p></blockquote><ol><li>IXSCAN（检索索引）<br>  耗费250ms,排查（examined）文档965978个，该步骤返回文档965978个，用到的索引是account_1_analysisId_1，该索引匹配到的数据965978.</li><li>FETCH（拉取数据）<br> 耗时660ms,排查（examined）文档965978个，该步骤返回文档１个，遍历加判断最终拿到想要的结果，这步耗时也是最多的一步。</li><li>KEEP_MUTATIONS（暂时不知道是干嘛的）</li></ol><blockquote><p>其实仔细看已经看出端倪了，这边提示的是Index account_1_analysisId_1 was used to find matching values for account,<strong>analysisId</strong>，还有looking for these criteria: {<strong>“analysis_id”.</strong>:{“$eq”:”bc70380f-47b0-40f9-9e24-f54d61ce8bd6”}}，复合索引中的字段并不是我们检索的mongo字段，后来翻查代码，</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Document</span>(collection = <span class="string">"bill_analysis_info"</span>)</div><div class="line"><span class="meta">@CompoundIndexes</span>(&#123; <span class="meta">@CompoundIndex</span>(name = <span class="string">"query_index"</span>, def = <span class="string">"&#123;account : 1, analysisId : 1&#125;"</span>) &#125;)</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BillAnalysisInfo</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;……&#125;</div></pre></td></tr></table></figure><blockquote><p>找到如上的根源。</p></blockquote><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><blockquote><p>手动建立针对这两个业务字段的复合索引</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">db.bill_analysis_info.ensureIndex(&#123;"account":1,"analysis_id":1&#125;)</div></pre></td></tr></table></figure><blockquote><p>再次查看同样查询的执行计划</p></blockquote><p><img src="/ITWO/assets/index_02.png" alt="重建索引执行计划给出执行过程"></p><blockquote><p><strong>效果很明显</strong></p></blockquote><h2 id="写在后面"><a href="#写在后面" class="headerlink" title="写在后面"></a>写在后面</h2><blockquote><p>mongo中原本以为会对建立索引过程中的字段是否为该集合的字段做判断，但此次排查下来明显没有，引以为戒。<br>细节，认真。<br>可视化工具用到的是dbKoda，要求mongo3.0版本以上。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;线上建模结果缓存在mongo中，之前由于数据量较少，进来一段时间由于老系统业务迁移过来，数据量也上来了，随之而来暴露出来一个问题.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="shell" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/shell/"/>
    
      <category term="mongo" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/mongo/"/>
    
      <category term="dbKoda" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/dbKoda/"/>
    
  </entry>
  
  <entry>
    <title>hadoop streaming 配置文件和字典文件分发方式</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/05/10/hadoop-streaming-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%92%8C%E5%AD%97%E5%85%B8%E6%96%87%E4%BB%B6%E5%88%86%E5%8F%91%E6%96%B9%E5%BC%8F/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/05/10/hadoop-streaming-配置文件和字典文件分发方式/</id>
    <published>2018-05-10T12:48:07.000Z</published>
    <updated>2018-05-14T01:48:11.952Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景描述"><a href="#背景描述" class="headerlink" title="背景描述"></a>背景描述</h2><blockquote><p>通常情况下我们在hadoop streaming中通过input来设置我们需要处理的数据,然后逐条遍历打标记,从而在reduce端可以区别对待做聚合或者join操作.但是我们需要用到一些配置文件怎么办?</p></blockquote><a id="more"></a><h2 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h2><blockquote><p>这里使用python来实现,我们的第一思路是:</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">python test.py dict.txt</div><div class="line">-file dict.txt</div></pre></td></tr></table></figure><blockquote><p>当然这种方案只可以针对本地文件,一方面这样的文件每次都要分发带来很大的局限性(如果文件很大怎么办?是不是会很耗费性能?显然是的),另一方面如果我们需要使用的文件在hdfs上怎么实施?<br>先贴下官网的普及文档</p></blockquote><h3 id="Hadoop-Streaming中的大文件和档案"><a href="#Hadoop-Streaming中的大文件和档案" class="headerlink" title="Hadoop Streaming中的大文件和档案"></a>Hadoop Streaming中的大文件和档案</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">任务使用-cacheFile和-cacheArchive选项在集群中分发文件和档案，选项的参数是用户已上传至HDFS的文件或档案的URI。这些文件和档案在不同的作业间缓存。用户可以通过fs.default.name.config配置参数的值得到文件所在的host和fs_port。</div><div class="line"></div><div class="line">这个是使用-cacheFile选项的例子：</div><div class="line"></div><div class="line">-cacheFile hdfs://host:fs_port/user/testfile.txt#testlink</div><div class="line">在上面的例子里，url中#后面的部分是建立在任务当前工作目录下的符号链接的名字。这里的任务的当前工作目录下有一个“testlink”符号链接，它指向testfile.txt文件在本地的拷贝。如果有多个文件，选项可以写成：</div><div class="line"></div><div class="line">-cacheFile hdfs://host:fs_port/user/testfile1.txt#testlink1 -cacheFile hdfs://host:fs_port/user/testfile2.txt#testlink2</div><div class="line">-cacheArchive选项用于把jar文件拷贝到任务当前工作目录并自动把jar文件解压缩。例如：</div><div class="line"></div><div class="line">-cacheArchive hdfs://host:fs_port/user/testfile.jar#testlink3</div><div class="line">在上面的例子中，testlink3是当前工作目录下的符号链接，它指向testfile.jar解压后的目录。</div><div class="line"></div><div class="line">下面是使用-cacheArchive选项的另一个例子。其中，input.txt文件有两行内容，分别是两个文件的名字：testlink/cache.txt和testlink/cache2.txt。“testlink”是指向档案目录（jar文件解压后的目录）的符号链接，这个目录下有“cache.txt”和“cache2.txt”两个文件。</div><div class="line"></div><div class="line"><span class="meta">$</span><span class="bash">HADOOP_HOME/bin/hadoop  jar <span class="variable">$HADOOP_HOME</span>/hadoop-streaming.jar \</span></div><div class="line">                  -input "/user/me/samples/cachefile/input.txt"  \</div><div class="line">                  -mapper "xargs cat"  \</div><div class="line">                  -reducer "cat"  \</div><div class="line">                  -output "/user/me/samples/cachefile/out" \  </div><div class="line">                  -cacheArchive 'hdfs://hadoop-nn1.example.com/user/me/samples/cachefile/cachedir.jar#testlink' \  </div><div class="line">                  -jobconf mapred.map.tasks=1 \</div><div class="line">                  -jobconf mapred.reduce.tasks=1 \ </div><div class="line">                  -jobconf mapred.job.name="Experiment"</div><div class="line"></div><div class="line"><span class="meta">$</span><span class="bash"> ls test_jar/</span></div><div class="line">cache.txt  cache2.txt</div><div class="line"></div><div class="line"><span class="meta">$</span><span class="bash"> jar cvf cachedir.jar -C test_jar/ .</span></div><div class="line">added manifest</div><div class="line">adding: cache.txt(in = 30) (out= 29)(deflated 3%)</div><div class="line">adding: cache2.txt(in = 37) (out= 35)(deflated 5%)</div><div class="line"></div><div class="line"><span class="meta">$</span><span class="bash"> hadoop dfs -put cachedir.jar samples/cachefile</span></div><div class="line"></div><div class="line"><span class="meta">$</span><span class="bash"> hadoop dfs -cat /user/me/samples/cachefile/input.txt</span></div><div class="line">testlink/cache.txt</div><div class="line">testlink/cache2.txt</div><div class="line"></div><div class="line"><span class="meta">$</span><span class="bash"> cat test_jar/cache.txt </span></div><div class="line">This is just the cache string</div><div class="line"></div><div class="line"><span class="meta">$</span><span class="bash"> cat test_jar/cache2.txt </span></div><div class="line">This is just the second cache string</div><div class="line"></div><div class="line"><span class="meta">$</span><span class="bash"> hadoop dfs -ls /user/me/samples/cachefile/out      </span></div><div class="line">Found 1 items</div><div class="line">/user/me/samples/cachefile/out/part-00000 </div><div class="line"></div><div class="line"><span class="meta">$</span><span class="bash"> hadoop dfs -cat /user/me/samples/cachefile/out/part-00000</span></div><div class="line">This is just the cache string   </div><div class="line">This is just the second cache string</div></pre></td></tr></table></figure><blockquote><p>以上的demo可以看出可以帮助我们通过这种途径去预览分发后的hdfs配置文件.</p></blockquote><h3 id="实践校验"><a href="#实践校验" class="headerlink" title="实践校验"></a>实践校验</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></div><div class="line"></div><div class="line">hadoop fs -rmr $&#123;2&#125;</div><div class="line"></div><div class="line">hadoop jar /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/hadoop-streaming-2.6.0-mr1-cdh5.4.0.jar \</div><div class="line">        -files testSymlink.py,hdfs:///user/upsmart/koulb/dict.txt#test \</div><div class="line"><span class="meta">#</span><span class="bash"> -file testSymlink.py \</span></div><div class="line"><span class="meta">#</span><span class="bash"> -cacheFile ,hdfs:///user/upsmart/koulb/dict.txt<span class="comment">#test \</span></span></div><div class="line">        -input $&#123;1&#125; \</div><div class="line">        -output $&#123;2&#125; \</div><div class="line">        -mapper "cat" \</div><div class="line">        -reducer "python testSymlink.py" \</div><div class="line">        -jobconf mapred.reduce.tasks=1 \</div><div class="line">        -jobconf mapred.job.name="testSymlink"</div></pre></td></tr></table></figure><blockquote><p>tips:此处的files等价于file + cacheFile,其实官网有很长一段时间的版本已经在推崇这样的做法,但是还是在兼容这样的模式用法,file用来上传分发本地的文件到集群,cacheFile顾名思义用的是集群上已经现存的资源.<br>-file option is deprecated, please use generic option -files instead.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/python</span></div><div class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></div><div class="line"></div><div class="line"><span class="comment"># 打开一个文件</span></div><div class="line"><span class="comment"># 通过软链接去访问配置文件</span></div><div class="line">fo = open(<span class="string">"test"</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">"文件名: "</span>, fo.name</div><div class="line"></div><div class="line"><span class="comment"># 遍历文件打印每行的数据</span></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> fo:</div><div class="line">        <span class="keyword">print</span> line.strip()</div><div class="line"></div><div class="line"><span class="comment"># 关闭打开的文件</span></div><div class="line">fo.close()</div></pre></td></tr></table></figure></p></blockquote><h2 id="细节描述"><a href="#细节描述" class="headerlink" title="细节描述"></a>细节描述</h2><h3 id="map使用细节"><a href="#map使用细节" class="headerlink" title="map使用细节"></a>map使用细节</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">-mapper "python testSymlink.py" \</div><div class="line">-jobconf mapred.reduce.tasks=0 \</div></pre></td></tr></table></figure><blockquote><p>看细节:</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">JobSubmitter: number of splits:3</div><div class="line"></div><div class="line"><span class="meta">#</span><span class="bash">  配置文件结果输出</span></div><div class="line">文件名:  test</div><div class="line">a</div><div class="line">b</div><div class="line">c</div><div class="line">d</div><div class="line">e</div><div class="line">文件名:  test</div><div class="line">a</div><div class="line">b</div><div class="line">c</div><div class="line">d</div><div class="line">e</div><div class="line">文件名:  test</div><div class="line">a</div><div class="line">b</div><div class="line">c</div><div class="line">d</div><div class="line">e</div></pre></td></tr></table></figure><blockquote><p>从上可以看出我们使用的配置文件打印了三次,和我么你的splits个数一致.也就是说和map个数一致.</p></blockquote><h3 id="recude使用细节"><a href="#recude使用细节" class="headerlink" title="recude使用细节"></a>recude使用细节</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">-reducer "python testSymlink.py" \</div><div class="line">-jobconf mapred.reduce.tasks=1 \</div></pre></td></tr></table></figure><blockquote><p>看细节</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> 配置文件输出</span></div><div class="line">文件名:  test</div><div class="line">a</div><div class="line">b</div><div class="line">c</div><div class="line">d</div><div class="line">e</div></pre></td></tr></table></figure><blockquote><p>同样可以得出输出的次数和reduce个数完全一致</p></blockquote><h3 id="使用总结"><a href="#使用总结" class="headerlink" title="使用总结"></a>使用总结</h3><blockquote><p>综上可得,完全达到了配置文件分发的目的,可以保证每个map和reduce都可以使用到所需的配置文件.</p><p>tips:<strong>JobSubmitter: number of splits:3</strong>,<br><strong>3</strong>  = <strong>files:2</strong> + <strong>input1</strong></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景描述&quot;&gt;&lt;a href=&quot;#背景描述&quot; class=&quot;headerlink&quot; title=&quot;背景描述&quot;&gt;&lt;/a&gt;背景描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;通常情况下我们在hadoop streaming中通过input来设置我们需要处理的数据,然后逐条遍历打标记,从而在reduce端可以区别对待做聚合或者join操作.但是我们需要用到一些配置文件怎么办?&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/python/"/>
    
      <category term="hadoop streaming" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hadoop-streaming/"/>
    
      <category term="cacheFile" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/cacheFile/"/>
    
      <category term="file" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/file/"/>
    
      <category term="files" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/files/"/>
    
  </entry>
  
  <entry>
    <title>chrome 浏览器页面乱码问题</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/05/09/chrome-%E6%B5%8F%E8%A7%88%E5%99%A8%E9%A1%B5%E9%9D%A2%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/05/09/chrome-浏览器页面乱码问题/</id>
    <published>2018-05-09T01:51:20.000Z</published>
    <updated>2018-05-09T02:16:27.818Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景描述"><a href="#背景描述" class="headerlink" title="背景描述"></a>背景描述</h2><blockquote><p>在做es分词的时候用到了ik分词器要用热分词,要用到一个web容器来搭建在线词库,遇到了以下的问题.<br><a id="more"></a></p><p>建立词库的时候原本打算是在服务器上建立的后来,无法确认它的编码格式,后来在本地用文本建立的词典,同时也另存为了utf-8格式,然后传到了tomcat服务器的ROOT目录下,然后启动在页面预览,但是出现了乱码.</p></blockquote><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><blockquote><p>原本下意识的解决思路是修改tomcat自带的server.xml配置,但是改了以后于事无补,在页面预览还是乱码.<br>后来考虑到是不是浏览器自己的编码格式问题,使用了curl直接访问这个文本静态文件,没有乱码的问题出现,从而定位去修改浏览器的默认格式问题,但是最新版本的chrome浏览器没有提供直接修改默认编码的通道,后来在chrome商店有搜到有以下这款插件可以解决问题.<br>ps:(A Google Chrome extension used to modify the page default encoding for Google Chrome 55+.)<br>55版本以后已经不支持在自定义字体里面修改.<br><a href="https://chrome.google.com/webstore/detail/oenllhgkiiljibhfagbfogdbchhdchml" target="_blank" rel="external">Charset</a></p></blockquote><h2 id="写在后面"><a href="#写在后面" class="headerlink" title="写在后面"></a>写在后面</h2><blockquote><p>后来发现其实跟server.xml的配置没关系,是不是新版本已经没有该问题了?(apache-tomcat-8.5.31)</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景描述&quot;&gt;&lt;a href=&quot;#背景描述&quot; class=&quot;headerlink&quot; title=&quot;背景描述&quot;&gt;&lt;/a&gt;背景描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;在做es分词的时候用到了ik分词器要用热分词,要用到一个web容器来搭建在线词库,遇到了以下的问题.&lt;br&gt;
    
    </summary>
    
    
      <category term="chrome" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/chrome/"/>
    
      <category term="ubantu" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/ubantu/"/>
    
      <category term="utf-8" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/utf-8/"/>
    
      <category term="tomcat" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/tomcat/"/>
    
  </entry>
  
  <entry>
    <title>linux 实用技巧总结</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/05/04/linux-%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/05/04/linux-实用技巧总结/</id>
    <published>2018-05-04T02:32:29.000Z</published>
    <updated>2018-05-14T01:54:10.693Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>总结以往在使用linux时,用到的一点小技巧或者命令集合</p></blockquote><a id="more"></a><h2 id="批量图片格式文件转换成一个pdf"><a href="#批量图片格式文件转换成一个pdf" class="headerlink" title="批量图片格式文件转换成一个pdf"></a>批量图片格式文件转换成一个pdf</h2><blockquote><p>convert *.jpg output.pdf<br>ps:就是这么任性</p></blockquote><h2 id="awk-按照某个key分发文件的妙用"><a href="#awk-按照某个key分发文件的妙用" class="headerlink" title="awk 按照某个key分发文件的妙用"></a>awk 按照某个key分发文件的妙用</h2><h3 id="需求背景："><a href="#需求背景：" class="headerlink" title="需求背景："></a>需求背景：</h3><blockquote><p>本来的需求是提取一个月的数据，但是出来以后产品又要拆分为每天的量<br><!--more--></p></blockquote><h3 id="技术实现"><a href="#技术实现" class="headerlink" title="技术实现"></a>技术实现</h3><blockquote><p>本来打算使用python foreach去解决，但是想到以前用过awk处理过类似的问题，乍一看日期后面还有时分秒，必然又用到了substr，妙的是awk也支持，脚本如下：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">awk -F <span class="string">','</span> <span class="string">'&#123;print $3  &gt;substr($2,1,10)".csv"&#125;'</span> sy*.txt;</div></pre></td></tr></table></figure><blockquote><p>完美的解决了我的问题，第二列是时间(带有时分秒,日期格式为2017-06-13的样式)，第三列为个人标示，唯一标示一条记录，当然你也可以使用$0，完成真正意义上的拆分文件。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">awk -F <span class="string">','</span> <span class="string">'&#123;print $0 &gt;substr($2,1,10)".csv"&#125;'</span>  sy*.txt;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;总结以往在使用linux时,用到的一点小技巧或者命令集合&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="linux" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/linux/"/>
    
      <category term="convert" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/convert/"/>
    
      <category term="awk" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/awk/"/>
    
  </entry>
  
  <entry>
    <title>mapreduce 结果集中在多个reduce输出part中的一个</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/27/mapreduce-%E7%BB%93%E6%9E%9C%E9%9B%86%E4%B8%AD%E5%9C%A8%E5%A4%9A%E4%B8%AAreduce%E8%BE%93%E5%87%BA%E4%B8%AD%E7%9A%84%E4%B8%80%E4%B8%AA/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/27/mapreduce-结果集中在多个reduce输出中的一个/</id>
    <published>2018-04-27T10:19:57.000Z</published>
    <updated>2018-05-14T01:54:46.531Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景描述"><a href="#背景描述" class="headerlink" title="背景描述"></a>背景描述</h2><blockquote><p>17G的输入数据经过map处理输出,map中只有打标记的处理,然后reduce只有cat操作,但是结果这些结果集中到一个part中</p></blockquote><a id="more"></a><h2 id="细节分析"><a href="#细节分析" class="headerlink" title="细节分析"></a>细节分析</h2><blockquote><p>先贴代码</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HashPartitioner</span><span class="params">()</span> </span>&#123;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</div><div class="line">        <span class="keyword">return</span> (key.hashCode() &amp; <span class="number">2147483647</span>) % numReduceTasks;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><blockquote><p>map.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">sep = <span class="string">"\t"</span></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">details = line.strip().split(sep)</div><div class="line">card = details[<span class="number">0</span>]</div><div class="line"><span class="keyword">print</span> card+sep+str(random.random())</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">main()</div></pre></td></tr></table></figure></p><p>start.sh</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/bin/bash</span></div><div class="line"></div><div class="line">hadoop fs -rmr $&#123;<span class="number">2</span>&#125;</div><div class="line">set -x</div><div class="line">hadoop jar /opt/cloudera/parcels/CDH<span class="number">-5.4</span><span class="number">.0</span><span class="number">-1.</span>cdh5<span class="number">.4</span><span class="number">.0</span>.p0<span class="number">.27</span>/jars/hadoop-streaming<span class="number">-2.6</span><span class="number">.0</span>-mr1-cdh5<span class="number">.4</span><span class="number">.0</span>.jar \</div><div class="line">        -input $&#123;<span class="number">1</span>&#125; \</div><div class="line">        -output $&#123;<span class="number">2</span>&#125; \</div><div class="line">        -file map.py \</div><div class="line">        -mapper <span class="string">"python map.py"</span> \</div><div class="line">        -reducer <span class="string">"cat"</span> \</div><div class="line">        -jobconf mapred.reduce.tasks=$&#123;<span class="number">3</span>&#125; \</div><div class="line">        -jobconf mapred.job.name=<span class="string">"reduce_test"</span></div></pre></td></tr></table></figure><blockquote><p>根据默认使用的HashPartitioner可以得出是不是key分布在同一个分区中取决于reduce个数和hashcode,所以我们需要做的就是实现一个程序计算得出这些key是不是会被放到同一个分区中.</p><p>getCode.py</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding:utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_n_bytes</span><span class="params">(n, b)</span>:</span></div><div class="line">    bits = b*<span class="number">8</span></div><div class="line">    <span class="keyword">return</span> (n + <span class="number">2</span>**(bits<span class="number">-1</span>)) % <span class="number">2</span>**bits - <span class="number">2</span>**(bits<span class="number">-1</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_4_bytes</span><span class="params">(n)</span>:</span></div><div class="line">    <span class="keyword">return</span> convert_n_bytes(n, <span class="number">4</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHashCode</span><span class="params">(s)</span>:</span></div><div class="line">    h = <span class="number">0</span></div><div class="line">    n = len(s)</div><div class="line">    <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(s):</div><div class="line">        h = h + ord(c)*<span class="number">31</span>**(n<span class="number">-1</span>-i)</div><div class="line">    <span class="keyword">return</span> convert_4_bytes(h)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">hashCode = getHashCode(line.strip())</div><div class="line"></div><div class="line"><span class="keyword">print</span>  (hashCode &amp; <span class="number">2147483647</span>) % reduceNum</div></pre></td></tr></table></figure><blockquote><p>经过以上的代码验证得出这些key确实是会被放到同一个partition中,后来了解到同事用的是上一个mapreduce计算逻辑的输出中的一个part作为输入,所以也就可以捋顺之前为什么会集中到reduce输出结果中的某一个part中而不分散的缘故.</p></blockquote><h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><blockquote><p>由此可见这样的方法,我们可以预估到这些key是不是交给同一个reduce去处理,但是并不能准确定位到它会出现在reduce输出结果的那个part中,只会是按照排序之后依次向下排序.所以如果所有的key最终如果只会交给一个reduce处理它的结果就只会出现在part0000中.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景描述&quot;&gt;&lt;a href=&quot;#背景描述&quot; class=&quot;headerlink&quot; title=&quot;背景描述&quot;&gt;&lt;/a&gt;背景描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;17G的输入数据经过map处理输出,map中只有打标记的处理,然后reduce只有cat操作,但是结果这些结果集中到一个part中&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="mapreduce" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/mapreduce/"/>
    
      <category term="part" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/part/"/>
    
      <category term="reduce" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/reduce/"/>
    
  </entry>
  
  <entry>
    <title>spring 优雅地关闭程序</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/27/spring-%E4%BC%98%E9%9B%85%E5%9C%B0%E5%85%B3%E9%97%AD%E7%A8%8B%E5%BA%8F/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/27/spring-优雅地关闭程序/</id>
    <published>2018-04-27T09:11:06.000Z</published>
    <updated>2018-05-14T01:55:27.826Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><blockquote><p>如何优雅地退出spring容器环境?</p></blockquote><a id="more"></a><h2 id="细节描述"><a href="#细节描述" class="headerlink" title="细节描述"></a>细节描述</h2><h3 id="Spring关闭钩子"><a href="#Spring关闭钩子" class="headerlink" title="Spring关闭钩子"></a>Spring关闭钩子</h3><blockquote><p>Spring在AbstractApplicationContext里维护了一个shutdownHook属性，用来关闭Spring上下文。但这个钩子不是默认生效的，需要手动调用&gt;ApplicationContext.registerShutdownHook()来开启，在自行维护ApplicationContext（而不是托管给tomcat之类的容器时），注意尽量使用&gt;ApplicationContext.registerShutdownHook()或者手动调用ApplicationContext.close()来关闭Spring上下文，否则应用退出时可能会残留资源。</p></blockquote><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><blockquote><p>Runtime.getRuntime().addShutdownHook的方法的意思就是在jvm中增加一个关闭的钩子，当jvm关闭的时候，会执行系统中已设置的所有通过addShutdownHook添加的钩子，当系统执行完这些钩子后，jvm才会关闭。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;如何优雅地退出spring容器环境?&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="spring" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/spring/"/>
    
      <category term="registerShutdownHook" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/registerShutdownHook/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop HDFS 数据自动平衡</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/27/%20Hadoop%20HDFS%20%E6%95%B0%E6%8D%AE%E8%87%AA%E5%8A%A8%E5%B9%B3%E8%A1%A1/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/27/ Hadoop HDFS 数据自动平衡/</id>
    <published>2018-04-27T05:53:42.269Z</published>
    <updated>2018-04-27T05:53:42.269Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Hadoop-HDFS-数据自动平衡脚本使用方法"><a href="#Hadoop-HDFS-数据自动平衡脚本使用方法" class="headerlink" title="Hadoop HDFS 数据自动平衡脚本使用方法"></a>Hadoop HDFS 数据自动平衡脚本使用方法</h2><blockquote><p>在Hadoop中，包含一个start-balancer.sh脚本，通过运行这个工具，启动HDFS数据均衡服务。该工具可以做到热插拔，即无须重启计算机和 Hadoop 服务。HadoopHome/bin目录下的start−balancer.sh脚本就是该任务的启动脚本。启动命令为：‘HadoopHome/bin目录下的start−balancer.sh脚本就是该任务的启动脚本。启动命令为：‘Hadoop_home/bin/start-balancer.sh –threshold`<br><a id="more"></a></p></blockquote><h2 id="影响Balancer的几个参数："><a href="#影响Balancer的几个参数：" class="headerlink" title="影响Balancer的几个参数："></a>影响Balancer的几个参数：</h2><p>-threshold</p><blockquote><p>默认设置：10，参数取值范围：0-100<br>参数含义：判断集群是否平衡的阈值。理论上，该参数设置的越小，整个集群就越平衡<br>dfs.balance.bandwidthPerSec<br>默认设置：1048576（1M/S）<br>参数含义：Balancer运行时允许占用的带宽<br>示例如下：</p></blockquote><p>#启动数据均衡，默认阈值为 10%<br>$Hadoop_home/bin/start-balancer.sh</p><p>#启动数据均衡，阈值 5%<br>bin/start-balancer.sh –threshold 5</p><p>#停止数据均衡<br>$Hadoop_home/bin/stop-balancer.sh<br>在hdfs-site.xml文件中可以设置数据均衡占用的网络带宽限制</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.balance.bandwidthPerSec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1048576<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span> Specifies the maximum bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second. <span class="tag">&lt;/<span class="name">description</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Hadoop-HDFS-数据自动平衡脚本使用方法&quot;&gt;&lt;a href=&quot;#Hadoop-HDFS-数据自动平衡脚本使用方法&quot; class=&quot;headerlink&quot; title=&quot;Hadoop HDFS 数据自动平衡脚本使用方法&quot;&gt;&lt;/a&gt;Hadoop HDFS 数据自动平衡脚本使用方法&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;在Hadoop中，包含一个start-balancer.sh脚本，通过运行这个工具，启动HDFS数据均衡服务。该工具可以做到热插拔，即无须重启计算机和 Hadoop 服务。HadoopHome/bin目录下的start−balancer.sh脚本就是该任务的启动脚本。启动命令为：‘HadoopHome/bin目录下的start−balancer.sh脚本就是该任务的启动脚本。启动命令为：‘Hadoop_home/bin/start-balancer.sh –threshold`&lt;br&gt;
    
    </summary>
    
    
      <category term="hadoop" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hadoop/"/>
    
      <category term="hdfs" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hdfs/"/>
    
      <category term="balancer" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/balancer/"/>
    
  </entry>
  
  <entry>
    <title>python utc时间转换为本地时间</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/27/python-utc%E6%97%B6%E9%97%B4%E8%BD%AC%E6%8D%A2%E4%B8%BA%E6%9C%AC%E5%9C%B0%E6%97%B6%E9%97%B4/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/27/python-utc时间转换为本地时间/</id>
    <published>2018-04-27T02:13:35.000Z</published>
    <updated>2018-05-02T01:30:08.350Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景描述"><a href="#背景描述" class="headerlink" title="背景描述"></a>背景描述</h2><blockquote><p>公司业务mongo库中,存了之前很多日志,但是日志中时间没有预处理,也就是说用的是默认的utc时间,我们现实中用到的东八区的时间,公司报表系统要用到这个业务字段,导出日志的时候用的是mongoexport,后续要对时间做处理,想到了用python脚本来二次处理.</p></blockquote><a id="more"></a><h2 id="技术实现"><a href="#技术实现" class="headerlink" title="技术实现"></a>技术实现</h2><blockquote><p>因为mongo导出的时间都是字符串,所以预先要考虑的是字符串格式的数据转换为时间格式的,然后对该时间加八小时格式化输出即可.</p></blockquote><h2 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节:"></a>技术细节:</h2><blockquote><p>先贴代码为敬:</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> dateutil <span class="keyword">import</span> parser</div><div class="line"><span class="keyword">import</span> datetime</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">details = line.strip().split(<span class="string">","</span>)</div><div class="line">time_string = details[<span class="number">0</span>]</div><div class="line">datetime_struct = parser.parse(time_string)</div><div class="line"><span class="comment"># print type(datetime_struct)</span></div><div class="line">o = datetime.timedelta(hours=<span class="number">8</span>)</div><div class="line">details[<span class="number">0</span>] = (datetime_struct+o).strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">"\t"</span>.join(details)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">main()</div></pre></td></tr></table></figure><blockquote><p>字符串时间转为时间格式用到的是dateutil里面parser,是不是很像java里面的?这个模块不是Python自带,我们需要使用pip安装名称如下</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo pip install python-dateutil</div></pre></td></tr></table></figure><blockquote><p>然后使用datetime中的timedelta函数格式化八小时增量模块然后与之前的格式化的时间做相加操作,得到最终要的东八区时间.</p></blockquote><h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><blockquote><p>如何打印出从某个日期往后顺延指定个数的日期,枚举出来.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> datetime</div><div class="line">start = datetime.datetime(<span class="number">2017</span>,<span class="number">10</span>,<span class="number">15</span>)</div><div class="line">zone = datetime.timedelta(days=<span class="number">1</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">184</span>):</div><div class="line">    day = start + zone*i</div><div class="line">    <span class="keyword">print</span> datetime.datetime.strftime(day,<span class="string">"%Y-%m-%d"</span>)</div></pre></td></tr></table></figure><blockquote><p>思路:推算之前大概预估下你要统计多久的顺延的日子,然后使用timedelta以此累加,遍历打印就是最终想要的结果.</p><p>ps:以此类推:某个特定小时,某个特定的月份,往后顺延,大同小异.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景描述&quot;&gt;&lt;a href=&quot;#背景描述&quot; class=&quot;headerlink&quot; title=&quot;背景描述&quot;&gt;&lt;/a&gt;背景描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;公司业务mongo库中,存了之前很多日志,但是日志中时间没有预处理,也就是说用的是默认的utc时间,我们现实中用到的东八区的时间,公司报表系统要用到这个业务字段,导出日志的时候用的是mongoexport,后续要对时间做处理,想到了用python脚本来二次处理.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/python/"/>
    
      <category term="utc" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/utc/"/>
    
      <category term="dateutil" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/dateutil/"/>
    
  </entry>
  
  <entry>
    <title>hexo yilla 和github 结合搭建个人博客</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/16/hexo%20yilia%20github%20%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/16/hexo yilia github 搭建个人博客/</id>
    <published>2018-04-16T03:17:28.107Z</published>
    <updated>2018-04-16T03:17:28.083Z</updated>
    
    <content type="html"><![CDATA[<h1 id="为什么考虑这样的搭配方式"><a href="#为什么考虑这样的搭配方式" class="headerlink" title="为什么考虑这样的搭配方式?"></a>为什么考虑这样的搭配方式?</h1><h2 id="构建需求"><a href="#构建需求" class="headerlink" title="构建需求"></a>构建需求</h2><blockquote><p>现阶段有很多的技术网站都带有给想要展示自己的一些技术入门以及技术研究的平台,也就常见的技术博客.<br>  但是大多都不满足于个性化定制,偶然间接触到markdown,当然简书等这样的平台也支持markdown,但是出于个人的独占情节,还是更倾向于搭建一个独立的自己可控的blog.<br><a id="more"></a></p><h2 id="技术实现-快速搭建"><a href="#技术实现-快速搭建" class="headerlink" title="技术实现(快速搭建)"></a>技术实现(快速搭建)</h2><p>考虑到如果从零开始,买空间,选域名,构建主体框架,渲染静态页面,一套走下来,未免本末倒置,博客注重的应该是文章的可读性以及质量,当然ui需要一定的可观瞻性.幸运的是遇到了hexo,给人一种转角遇到爱的小确幸.<br>      ps:Hexo is a fast, simple &amp; powerful blog framework powered by Node.js.<br>      从官网的解释可以看出,我们需要安装Node.js,当然要和gitbub结合,你需要申请一个github账号,申请账号的步骤,此处就不再累述.<br>      笔者使用的系统是ubantu 16.04</p><p>传送门:<a href="https://github.com/join" target="_blank" rel="external">github账号申请</a></p></blockquote><h3 id="现在演示安装Node-js"><a href="#现在演示安装Node-js" class="headerlink" title="现在演示安装Node.js."></a>现在演示安装Node.js.</h3><blockquote><p>在 Github 上获取 Node.js 源码：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo git <span class="built_in">clone</span> https://github.com/nodejs/node.git</div></pre></td></tr></table></figure><p> 1.修改目录权限：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo chmod -R 755 node</div><div class="line">$ <span class="built_in">cd</span> node</div><div class="line">$ node -v</div></pre></td></tr></table></figure></p><p> 2.使用 ./configure 创建编译文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo ./configure</div></pre></td></tr></table></figure></p><p> 3.这一步，可能时间有点长，耐心等待<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo make</div></pre></td></tr></table></figure></p><p> 4.最后<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo make</div></pre></td></tr></table></figure></p><blockquote><p>install 查看版本</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ node -v</div></pre></td></tr></table></figure><blockquote><p>v0.10.25 如果node不是最新的，node有一个模块叫n，是专门用来管理node.js的版本的。使用npm（NPM是随同nodejs一起安装的包管理工具）安装n模块</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo npm install -g n</div></pre></td></tr></table></figure><blockquote><p>然后，升级node.js到最新稳定版</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo n stable</div></pre></td></tr></table></figure><blockquote><p>旧版本的 npm，也可以很容易地通过 npm 命令来升级，命令如下：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo npm install npm -g</div></pre></td></tr></table></figure><h3 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h3><blockquote><p>执行以下的命令:</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-cli -g</div><div class="line">$ hexo init blog 此处会新建一个新的目录存储hexo的一些初始化的文件.</div><div class="line">$ <span class="built_in">cd</span> blog</div><div class="line">$ npm install</div><div class="line">$ hexo server 此处会生成一个新的本地预览 访问http://localhost:4000 就可以访问本地的默认主题.</div></pre></td></tr></table></figure><h3 id="新建github仓库"><a href="#新建github仓库" class="headerlink" title="新建github仓库"></a>新建github仓库</h3><blockquote><p>新建一个仓库,然后选择public权限(写博客不就是为了别人看,进而监督自己进步么,所以public),指定git分支,使用默认的master分支即可,因为这个仓库就你一个看门的,这里面也是你的.然后记住自己的clone地址.<br>ps:记得保存.</p></blockquote><h3 id="将gitbub仓库和hexo主题绑定"><a href="#将gitbub仓库和hexo主题绑定" class="headerlink" title="将gitbub仓库和hexo主题绑定"></a>将gitbub仓库和hexo主题绑定</h3><blockquote><p>编辑_config.yml:</p></blockquote><figure class="highlight less"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="attribute">type</span>: git</div><div class="line"><span class="attribute">repo</span>: <span class="attribute">https</span>:<span class="comment">//github.com/tangyuxiaoyao/ITWO.git</span></div><div class="line"><span class="attribute">branch</span>: master</div></pre></td></tr></table></figure><blockquote><p>repo配置的地址为上文已经提及过的项目仓库clone地址.<br>而且这里有看到仓库后面带有子资源路径所以参考配置文件中的注释,需要将root对应的配置改为仓库的名称资源路径.</p></blockquote><figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># URL</div><div class="line">## If your site <span class="keyword">is</span> <span class="keyword">put</span> in <span class="keyword">a</span> subdirectory, <span class="keyword">set</span> url <span class="keyword">as</span> <span class="string">'http://yoursite.com/child'</span> <span class="built_in">and</span> root <span class="keyword">as</span> <span class="string">'/child/'</span></div><div class="line">roo<span class="variable">t:</span> /ITWO/</div></pre></td></tr></table></figure><h3 id="发布主题到github仓库"><a href="#发布主题到github仓库" class="headerlink" title="发布主题到github仓库"></a>发布主题到github仓库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">npm install hexo-deployer-git --save</div><div class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</div></pre></td></tr></table></figure><ol><li>安装hexo发布模块deploy</li><li>清除缓存</li><li>生成静态页面</li><li>发布到github(每次改完以后也是这么稳妥发布)<h3 id="生成新的文章"><a href="#生成新的文章" class="headerlink" title="生成新的文章"></a>生成新的文章</h3></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> <span class="built_in">source</span>/_posts/</div><div class="line">$ hexo new <span class="string">"shell在指定行插入文本"</span></div></pre></td></tr></table></figure><blockquote><p>然后就会生成一个为该名称的md文件,根据md语法编辑内容,完成以后发布即可.<br>ps:可以用hexo clean &amp;&amp; hexo g &amp;&amp; hexo s在本地生成预览效果,避免发布到github上的效果不尽人意.</p></blockquote><p><img src="/ITWO/assets/jscy.jpg" alt="古人笑比庭中树,一日秋风一日疏"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;为什么考虑这样的搭配方式&quot;&gt;&lt;a href=&quot;#为什么考虑这样的搭配方式&quot; class=&quot;headerlink&quot; title=&quot;为什么考虑这样的搭配方式?&quot;&gt;&lt;/a&gt;为什么考虑这样的搭配方式?&lt;/h1&gt;&lt;h2 id=&quot;构建需求&quot;&gt;&lt;a href=&quot;#构建需求&quot; class=&quot;headerlink&quot; title=&quot;构建需求&quot;&gt;&lt;/a&gt;构建需求&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;现阶段有很多的技术网站都带有给想要展示自己的一些技术入门以及技术研究的平台,也就常见的技术博客.&lt;br&gt;  但是大多都不满足于个性化定制,偶然间接触到markdown,当然简书等这样的平台也支持markdown,但是出于个人的独占情节,还是更倾向于搭建一个独立的自己可控的blog.&lt;br&gt;
    
    </summary>
    
    
      <category term="hexo" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hexo/"/>
    
      <category term="yilla" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/yilla/"/>
    
      <category term="github" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>如何在hadoop streaming mapreduce中 使用python第三方包</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/11/%E5%A6%82%E4%BD%95%E5%9C%A8hadoop-streaming-mapreduce%E4%B8%AD-%E4%BD%BF%E7%94%A8python%E7%AC%AC%E4%B8%89%E6%96%B9%E5%8C%85/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/11/如何在hadoop-streaming-mapreduce中-使用python第三方包/</id>
    <published>2018-04-11T09:26:09.000Z</published>
    <updated>2018-04-16T02:43:46.578Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景描述"><a href="#背景描述" class="headerlink" title="背景描述"></a>背景描述</h2><blockquote><p>当我们需要在大数据平台使用python第三方包的时候，第一印象是在集群的每台集群都安装该第三方Python包，但是这样需要集群重启，很明显此方案扑街。<br><a id="more"></a></p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>我们使用hadoop streaming提供你的压缩文件分发命令来使用我们的第三方Python包。</p></blockquote><h2 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h2><blockquote><p>我们先了解下该命令：</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">-archivesOptionalSpecify comma-separated archives to be unarchived on the compute machines</div></pre></td></tr></table></figure><blockquote><p>字面意思是上传多个以逗号分割的压缩包到集群然后会分发解压到各个计算节点。</p></blockquote><h3 id="打包"><a href="#打包" class="headerlink" title="打包"></a>打包</h3><blockquote><p>打包集成python和第三方python包。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> 需要使用第三方库如bs4,numpy等时，需要用到虚拟环境virtualenv</span></div><div class="line"></div><div class="line"><span class="meta">#</span><span class="bash"> virtualenv的使用</span></div><div class="line"><span class="meta">#</span><span class="bash"> 安装</span></div><div class="line"></div><div class="line">pip install virtualenv</div><div class="line"><span class="meta">#</span><span class="bash"> 新建虚拟环境</span></div><div class="line"></div><div class="line">virtualenv kxvp</div><div class="line"></div><div class="line"><span class="meta">#</span><span class="bash"> 使得虚拟环境的路径为相对路径</span></div><div class="line"></div><div class="line">virtualenv --relocatable kxvp</div><div class="line"><span class="meta">#</span><span class="bash"> 激活虚拟环境</span></div><div class="line"></div><div class="line">source kxvp/bin/activate</div><div class="line"><span class="meta">#</span><span class="bash"> 如果想退出，可以使用下面的命令</span></div><div class="line"></div><div class="line">deactivate</div><div class="line"><span class="meta">#</span><span class="bash"> 激活后直接安装各种需要的包</span></div><div class="line"></div><div class="line"><span class="meta">#</span><span class="bash"> pip install XXX</span></div><div class="line"><span class="meta">#</span><span class="bash"> 压缩环境包</span></div><div class="line"></div><div class="line">tar -czf kxvp.tar.gz kxvp</div></pre></td></tr></table></figure><blockquote><p>上传到集群客户端</p></blockquote><h3 id="编写测试脚本"><a href="#编写测试脚本" class="headerlink" title="编写测试脚本"></a>编写测试脚本</h3><blockquote><p>test.sh</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></div><div class="line"></div><div class="line">hadoop fs -rmr $&#123;2&#125;</div><div class="line"></div><div class="line">hadoop jar /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/hadoop-streaming-2.6.0-cdh5.4.0.jar \</div><div class="line">        -input $&#123;1&#125;\</div><div class="line">        -output $&#123;2&#125; \</div><div class="line">        -file map.py \</div><div class="line">        -mapper "kx/kxvp/bin/python map.py" \</div><div class="line">-cacheArchive '/user/upsmart/koulb/kxvp.tar.gz#kx'\</div><div class="line">        -jobconf mapred.reduce.tasks=0 \</div><div class="line">        -jobconf mapred.job.name="cache_archive_test"</div></pre></td></tr></table></figure><blockquote><p>map.py</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(separator = <span class="string">','</span>)</span>:</span></div><div class="line"><span class="keyword">print</span> np.__version__</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure><blockquote><p>tip:Python的执行路径注意要加上#后面的前缀</p></blockquote><h3 id="结果验证"><a href="#结果验证" class="headerlink" title="结果验证"></a>结果验证</h3><blockquote><p>看下集群客户端的numpy版本</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">upsmart@cluster-8:~/test_tmp$ python</div><div class="line">Python 2.7.6 (default, Oct 26 2016, 20:30:19) </div><div class="line">[GCC 4.8.4] on linux2</div><div class="line">Type "help", "copyright", "credits" or "license" for more information.</div><div class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import numpy</span></div><div class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="built_in">print</span> numpy.__version__</span></div><div class="line">1.8.2</div></pre></td></tr></table></figure><blockquote><p>看下mapreduce输出的结果</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">upsmart@cluster-8:~/test_tmp$ hadoop fs -getmerge koulb/test/p* rs</div><div class="line">upsmart@cluster-8:~/test_tmp$ more rs</div><div class="line">1.14.2</div></pre></td></tr></table></figure><blockquote><p>不一样验证通过.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景描述&quot;&gt;&lt;a href=&quot;#背景描述&quot; class=&quot;headerlink&quot; title=&quot;背景描述&quot;&gt;&lt;/a&gt;背景描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;当我们需要在大数据平台使用python第三方包的时候，第一印象是在集群的每台集群都安装该第三方Python包，但是这样需要集群重启，很明显此方案扑街。&lt;br&gt;
    
    </summary>
    
    
      <category term="hadoop" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hadoop/"/>
    
      <category term="streaming" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/streaming/"/>
    
      <category term="python" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/python/"/>
    
      <category term="mapreduce" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/mapreduce/"/>
    
      <category term="numpy" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>shell 遍历多级目录取文件的第一行</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/08/shell-%E9%81%8D%E5%8E%86%E5%A4%9A%E7%BA%A7%E7%9B%AE%E5%BD%95%E5%8F%96%E6%96%87%E4%BB%B6%E7%9A%84%E7%AC%AC%E4%B8%80%E8%A1%8C/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/08/shell-遍历多级目录取文件的第一行/</id>
    <published>2018-04-08T11:45:10.000Z</published>
    <updated>2018-04-16T02:44:03.373Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><blockquote><p>数据取样，从当前目录下去遍历多级目录拿到每个文件的第一行数据输出到一个文件中。<br><a id="more"></a></p></blockquote><h2 id="技术分析"><a href="#技术分析" class="headerlink" title="技术分析"></a>技术分析</h2><blockquote><p>当听到该需求时，注意点有两点</p></blockquote><ol><li>多级目录</li><li>取文件的第一行</li></ol><blockquote><p>for循环去递归遍历</p></blockquote><h2 id="技术实现"><a href="#技术实现" class="headerlink" title="技术实现"></a>技术实现</h2><blockquote><p>viewFile.sh<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></div><div class="line">function getdir()&#123;</div><div class="line">    for element in `ls $1`</div><div class="line">    do  </div><div class="line">        dir_or_file=$1"/"$element</div><div class="line">        if [ -d $dir_or_file ]</div><div class="line">        then </div><div class="line">            getdir $dir_or_file</div><div class="line">        else</div><div class="line">           head -1 $dir_or_file</div><div class="line">        fi  </div><div class="line">    done</div><div class="line">&#125;</div><div class="line">root_dir="/home/test/multiFolder"</div><div class="line">getdir $root_dir</div></pre></td></tr></table></figure></p><p>代码如上请笑纳。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求背景&quot;&gt;&lt;a href=&quot;#需求背景&quot; class=&quot;headerlink&quot; title=&quot;需求背景&quot;&gt;&lt;/a&gt;需求背景&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;数据取样，从当前目录下去遍历多级目录拿到每个文件的第一行数据输出到一个文件中。&lt;br&gt;
    
    </summary>
    
    
      <category term="shell" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/shell/"/>
    
      <category term="file" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/file/"/>
    
  </entry>
  
  <entry>
    <title>mysql 和mongo 导入数据总结</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/04/mysql-%E5%92%8Cmongo-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E6%80%BB%E7%BB%93/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/04/mysql-和mongo-导入数据总结/</id>
    <published>2018-04-04T07:17:22.000Z</published>
    <updated>2018-04-16T02:43:57.221Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><blockquote><p>线上系统环境迁移，数据部分mysql和mongo需要迁移，大部分的数据通过dump，已经迁移过去，迁移过程中会有增量的数据产生，这部分数据要怎么做到无缝迁移。<br><a id="more"></a></p><h2 id="实现分析"><a href="#实现分析" class="headerlink" title="实现分析"></a>实现分析</h2><p>因为系统落地之前都会放到队列里面等待消费，然后落库，所以新系统保证服务正常以后，域名解析到新环境，然后流程环节知道队列，等待老版环境队列消费完了而且ngnix中无用户请求记录进来（因为域名解析会有几分钟的时间），我们开始提取增量的数据，mysql记录的是表记录的最大id，mongo记录也是最大的集合objectId，然后批量导出。</p></blockquote><h2 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h2><h3 id="mysql"><a href="#mysql" class="headerlink" title="mysql"></a>mysql</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">导出</span></div><div class="line">mysql -uupsmart -p dbname -e 'select * from t where id&gt;max(id)' &gt;new_data.tsv</div><div class="line"><span class="meta">#</span><span class="bash">导入</span></div><div class="line">mysqlimport [options] db_name textfile1 ...</div></pre></td></tr></table></figure><blockquote><p>tip①:max(id)为dump之前记录的最大id。<br>tip②:mysqllimport导入的时候一定要指定-local，当然这部分属于options，或者写的是绝对路径否则会报错，默认找的是和mysqlimport命令同一路径下面的文件<br>tip③:dbname紧贴要导入的文件，而且textfile1的前缀名即为表名（.之前的那货）</p></blockquote><h3 id="mongo"><a href="#mongo" class="headerlink" title="mongo"></a>mongo</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">导出</span></div><div class="line">mongoexport -d dbanme -c collname --type=json -o new_data.csv --query='&#123;"_id":"&#123;"$gt":ObjectId("max(id)")&#125;"&#125;' </div><div class="line"><span class="meta">#</span><span class="bash">导入</span></div><div class="line">mongoimport  -d dbanme -c collname  --type=json --file new_data.csv</div></pre></td></tr></table></figure><blockquote><p>tip:本来打算是导出的时候指定csv格式然后导出，但是使用mongexport制定–type=csv的时候不能导出全部的字段，后来选择了默认的json格式导出。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求背景&quot;&gt;&lt;a href=&quot;#需求背景&quot; class=&quot;headerlink&quot; title=&quot;需求背景&quot;&gt;&lt;/a&gt;需求背景&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;线上系统环境迁移，数据部分mysql和mongo需要迁移，大部分的数据通过dump，已经迁移过去，迁移过程中会有增量的数据产生，这部分数据要怎么做到无缝迁移。&lt;br&gt;
    
    </summary>
    
    
      <category term="mongo" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/mongo/"/>
    
      <category term="mysql" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>清洗流水之去除字段中多余的分隔符</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/03/30/%E6%B8%85%E6%B4%97%E6%B5%81%E6%B0%B4%E4%B9%8B%E5%8E%BB%E9%99%A4%E5%AD%97%E6%AE%B5%E4%B8%AD%E5%A4%9A%E4%BD%99%E7%9A%84%E5%88%86%E9%9A%94%E7%AC%A6/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/03/30/清洗流水之去除字段中多余的分隔符/</id>
    <published>2018-03-30T06:48:57.000Z</published>
    <updated>2018-04-24T08:05:46.797Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><blockquote><p>正常的公司流水是47个字段，在做ETL的时候发现里面用分隔符去切割流水处理发现有超过47长度的，毋庸置疑肯定是其中有个别字段中含有分隔符，后来定位到是商户地址中含有分隔符而且不规则会出现多个，现在需要做的是将商户名称中的分隔符替换为空，然后重新拼装成我们需要的流水输出。<br><a id="more"></a></p></blockquote><h2 id="技术实现"><a href="#技术实现" class="headerlink" title="技术实现"></a>技术实现</h2><blockquote><p>直接贴代码</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeDirtyData</span><span class="params">(sep=<span class="string">","</span>)</span>:</span></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">details = line.strip().split(sep,<span class="number">39</span>)</div><div class="line">bad_col = details[<span class="number">39</span>].split(sep)[:<span class="number">-7</span>]</div><div class="line"><span class="keyword">if</span>(len(bad_col)&gt;<span class="number">1</span>):</div><div class="line">fix_col = <span class="string">""</span>.join(bad_col)</div><div class="line"><span class="keyword">print</span> <span class="string">"%s,%s,%s"</span> % (sep.join(details[:<span class="number">39</span>]),fix_col,sep.join(details[<span class="number">39</span>].split(sep)[<span class="number">-7</span>:]))</div><div class="line"><span class="keyword">else</span>:</div><div class="line"><span class="keyword">print</span>  line.strip()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">removeDirtyData()</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">main()</div></pre></td></tr></table></figure><h2 id="细节分析"><a href="#细节分析" class="headerlink" title="细节分析"></a>细节分析</h2><ol><li>先定位到需要的截取字段之前的位置切割，然后把后面的部分单独处理。</li><li>后面部分的数据使用[:-num]的方式，拿到脏数据中的异常字段数组</li><li>通过join的特性用空替换掉里面的分隔符</li><li>分三部分把清洗好的流水拼接起来。</li><li>当然正常的数据不用处理正常输出。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求描述&quot;&gt;&lt;a href=&quot;#需求描述&quot; class=&quot;headerlink&quot; title=&quot;需求描述&quot;&gt;&lt;/a&gt;需求描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;正常的公司流水是47个字段，在做ETL的时候发现里面用分隔符去切割流水处理发现有超过47长度的，毋庸置疑肯定是其中有个别字段中含有分隔符，后来定位到是商户地址中含有分隔符而且不规则会出现多个，现在需要做的是将商户名称中的分隔符替换为空，然后重新拼装成我们需要的流水输出。&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/python/"/>
    
      <category term="join" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/join/"/>
    
      <category term="split" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/split/"/>
    
  </entry>
  
  <entry>
    <title>巧用hbase shell 批量提取数据</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/03/28/%E5%B7%A7%E7%94%A8hbase-shell-%E6%89%B9%E9%87%8F%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/03/28/巧用hbase-shell-批量提取数据/</id>
    <published>2018-03-28T06:47:09.000Z</published>
    <updated>2018-03-28T07:44:56.571Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><blockquote><p>有一批卡号的需求提取全量的指标数据，这批数据存在于hbase中，所以要考虑去做mapping，来拿到数据，首先考虑的是通过hive sql语句去做join拿到要的数据，当然也是最简单，但是该hbase集群没有装hive，就是这么意外，第二策略是使用Python访问hbase(happybase),但是也要安装很多东西，最后是使用hbase shell 拿到数据以后，再批量拼装。<br><a id="more"></a></p><h2 id="技术分析实现"><a href="#技术分析实现" class="headerlink" title="技术分析实现"></a>技术分析实现</h2></blockquote><h3 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> hive_hbase_test(<span class="keyword">key</span> <span class="built_in">int</span>,<span class="keyword">value</span> <span class="keyword">string</span>)</div><div class="line"><span class="keyword">stored</span> <span class="keyword">by</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span> <span class="keyword">with</span> serdeproperties(<span class="string">"hbase.columns.mapping"</span>=<span class="string">":key,cf1:val"</span>) tblproperties(<span class="string">"hbase.table.name"</span>=<span class="string">"hive_hbase_test"</span>);</div></pre></td></tr></table></figure><blockquote><p>在现有hbase数据的基础上，我们建立一张外表打通hive和hbase数据的映射关系，然后使用我们熟悉的sqljoin然后 通过 hive -e 导出到本地收工。</p></blockquote><h3 id="python"><a href="#python" class="headerlink" title="python"></a>python</h3><blockquote><p>此种方法没有试过，只是存在与理论中,可以参考此篇文章，优势:可以使用hbase连接池（因为是通过停thift协议通信，tcp通信）。<br><a href="https://my.oschina.net/wolfoxliu/blog/856175" target="_blank" rel="external">Hbase实战教程之happybase</a></p></blockquote><h3 id="hbase，shell-，python"><a href="#hbase，shell-，python" class="headerlink" title="hbase，shell ，python"></a>hbase，shell ，python</h3><blockquote><p>次篇幅的重点介绍的也是这种方法。</p></blockquote><ol><li>利用shell生成要提取的hbase shell语句<blockquote><p>利用shell 拼接字符的方式把所有的查询语句生成到一个文件中，当然如果需要查询的key较多你也可以多生成到几个文件中，此处不再赘述，进而使用hbase shell 语句文件得到解析到结果。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">FILENAME=$1</div><div class="line">cat $FILENAME | while read LINE</div><div class="line">do</div><div class="line">echo "exists '$LINE'" &gt;&gt;temp_get</div><div class="line">echo "get 'card_md5','$LINE'" &gt;&gt;temp_get</div><div class="line">done</div><div class="line">hbase shell temp_get &gt;UDcredit-20180124-1_card_res.csv</div><div class="line"></div><div class="line">if test $? -eq 0</div><div class="line">then</div><div class="line">exit</div><div class="line">fi</div></pre></td></tr></table></figure></blockquote></li></ol><blockquote><p>要点:因为hbase shell查询出的结果中不会有key，所以这里我们需要暂存下key到文件中，并且肯定是要伴随着结果的输出，这样便于我们处理。所以shell逻辑如上。</p></blockquote><ol><li>利用Python处理结果文件为我们要的kv形式</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"></div><div class="line">md5_aes=&#123;&#125;</div><div class="line"></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">detail = line.strip()</div><div class="line"><span class="keyword">if</span>(<span class="string">'Table'</span> <span class="keyword">in</span> detail):</div><div class="line">lines = detail.split(<span class="string">" "</span>)</div><div class="line">md5card = lines[<span class="number">1</span>]</div><div class="line">md5_aes[md5card]=<span class="string">''</span></div><div class="line"><span class="keyword">if</span>(<span class="string">"value"</span> <span class="keyword">in</span> detail):</div><div class="line">lines = detail.split(<span class="string">"value="</span>)</div><div class="line">sm3card = lines1</div><div class="line">md5_aes[md5card]=sm3card</div><div class="line"></div><div class="line"><span class="keyword">for</span> key <span class="keyword">in</span> md5_aes.keys():</div><div class="line"><span class="keyword">print</span> key+<span class="string">","</span>+md5_aes[key]</div></pre></td></tr></table></figure><blockquote><p>利用当前行的特殊关键字来分割当前行数据，然后利用字典，拼接kv输出得到我们要的结果。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求描述&quot;&gt;&lt;a href=&quot;#需求描述&quot; class=&quot;headerlink&quot; title=&quot;需求描述&quot;&gt;&lt;/a&gt;需求描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;有一批卡号的需求提取全量的指标数据，这批数据存在于hbase中，所以要考虑去做mapping，来拿到数据，首先考虑的是通过hive sql语句去做join拿到要的数据，当然也是最简单，但是该hbase集群没有装hive，就是这么意外，第二策略是使用Python访问hbase(happybase),但是也要安装很多东西，最后是使用hbase shell 拿到数据以后，再批量拼装。&lt;br&gt;
    
    </summary>
    
    
      <category term="shell" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/shell/"/>
    
      <category term="python" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/python/"/>
    
      <category term="hbase" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>nohup &amp; &gt; 使用解释</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/03/14/nohup-%E4%BD%BF%E7%94%A8%E8%A7%A3%E9%87%8A/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/03/14/nohup-使用解释/</id>
    <published>2018-03-14T01:54:12.000Z</published>
    <updated>2018-04-16T03:19:39.333Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景描述"><a href="#背景描述" class="headerlink" title="背景描述"></a>背景描述</h2><blockquote><p>我们在运行服务或者其他程序的时候经常这样启动进程。<a id="more"></a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nohup ls &gt;out.log 2&gt;&amp;1 &amp;</div></pre></td></tr></table></figure><h2 id="细节分析"><a href="#细节分析" class="headerlink" title="细节分析"></a>细节分析</h2><blockquote><p>&amp; 可以免疫SIGINT（sign of interuput），也就是说他可以用于程序后台运行，但是关闭当前shell session就会退出该程序。<br>nohup 的运用是为了免疫SIGHUP信号，可以避免挂起，将程序的输出重定向到nohup.out中，也就是说免疫回话级别的关闭，但是不免疫SIGINT信号，也就是我们所用的Ctrl+C。<br>综上所属我们需要做的就是结合两者的优点，可以这样使用从而真正意义上实现程序后台运行，而且免疫session关闭而导致程序退出。<br>2&gt;&amp;1 的运用是为了将标准出错重定向到标准输出。<br>nohup ls &gt;out.log 2&gt;&amp;1 &amp; 连接起来的意思就是后台挂起ls命令，并且将标准输出和标准出错都作为输出重定向到out.log中，我们可以使用tail -f out.log监控程序的运行过程。<br>tip:当当前session还为关闭，我们可以使用Jobs命令来查看当前会话下面有多少挂起的程序，如果想停止自己启动的程序，可以使用fg %jobnumber的方式，将该任务放到前台执行然后使用Ctrl+C命令终止它的执行。但是如果当前session已经失去，我们就只能通过ps -ef 找到提交的任务然后通过kill来结束程序。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景描述&quot;&gt;&lt;a href=&quot;#背景描述&quot; class=&quot;headerlink&quot; title=&quot;背景描述&quot;&gt;&lt;/a&gt;背景描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;我们在运行服务或者其他程序的时候经常这样启动进程。
    
    </summary>
    
    
      <category term="nohup" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/nohup/"/>
    
  </entry>
  
</feed>
