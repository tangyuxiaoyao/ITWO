<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ITWO</title>
  
  <subtitle>毋庸多言,只管前行.</subtitle>
  <link href="/ITWO/atom.xml" rel="self"/>
  
  <link href="https://www.tangyuxiaoyao.club/ITWO/"/>
  <updated>2018-04-28T03:09:17.825Z</updated>
  <id>https://www.tangyuxiaoyao.club/ITWO/</id>
  
  <author>
    <name>唐钰逍遥</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>mapreduce 结果集中在多个reduce输出part中的一个</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/27/mapreduce-%E7%BB%93%E6%9E%9C%E9%9B%86%E4%B8%AD%E5%9C%A8%E5%A4%9A%E4%B8%AAreduce%E8%BE%93%E5%87%BA%E4%B8%AD%E7%9A%84%E4%B8%80%E4%B8%AA/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/27/mapreduce-结果集中在多个reduce输出中的一个/</id>
    <published>2018-04-27T10:19:57.000Z</published>
    <updated>2018-04-28T03:09:17.825Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景描述"><a href="#背景描述" class="headerlink" title="背景描述"></a>背景描述</h2><blockquote><p>17G的输入数据经过map处理输出,map中只有打标记的处理,然后reduce只有cat操作,但是结果这些结果集中到一个part中</p></blockquote><a id="more"></a><h2 id="细节分析"><a href="#细节分析" class="headerlink" title="细节分析"></a>细节分析</h2><blockquote><p>先贴代码</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HashPartitioner</span><span class="params">()</span> </span>&#123;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</div><div class="line">        <span class="keyword">return</span> (key.hashCode() &amp; <span class="number">2147483647</span>) % numReduceTasks;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><blockquote><p>map.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">sep = <span class="string">"\t"</span></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">details = line.strip().split(sep)</div><div class="line">card = details[<span class="number">0</span>]</div><div class="line"><span class="keyword">print</span> card+sep+str(random.random())</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">main()</div></pre></td></tr></table></figure></p><p>start.sh</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/bin/bash</span></div><div class="line"></div><div class="line">hadoop fs -rmr $&#123;<span class="number">2</span>&#125;</div><div class="line">set -x</div><div class="line">hadoop jar /opt/cloudera/parcels/CDH<span class="number">-5.4</span><span class="number">.0</span><span class="number">-1.</span>cdh5<span class="number">.4</span><span class="number">.0</span>.p0<span class="number">.27</span>/jars/hadoop-streaming<span class="number">-2.6</span><span class="number">.0</span>-mr1-cdh5<span class="number">.4</span><span class="number">.0</span>.jar \</div><div class="line">        -input $&#123;<span class="number">1</span>&#125; \</div><div class="line">        -output $&#123;<span class="number">2</span>&#125; \</div><div class="line">        -file map.py \</div><div class="line">        -mapper <span class="string">"python map.py"</span> \</div><div class="line">        -reducer <span class="string">"cat"</span> \</div><div class="line">        -jobconf mapred.reduce.tasks=$&#123;<span class="number">3</span>&#125; \</div><div class="line">        -jobconf mapred.job.name=<span class="string">"reduce_test"</span></div></pre></td></tr></table></figure><blockquote><p>根据默认使用的HashPartitioner可以得出是不是key分布在同一个分区中取决于reduce个数和hashcode,所以我们需要做的就是实现一个程序计算得出这些key是不是会被放到同一个分区中.</p><p>getCode.py</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding:utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_n_bytes</span><span class="params">(n, b)</span>:</span></div><div class="line">    bits = b*<span class="number">8</span></div><div class="line">    <span class="keyword">return</span> (n + <span class="number">2</span>**(bits<span class="number">-1</span>)) % <span class="number">2</span>**bits - <span class="number">2</span>**(bits<span class="number">-1</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_4_bytes</span><span class="params">(n)</span>:</span></div><div class="line">    <span class="keyword">return</span> convert_n_bytes(n, <span class="number">4</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHashCode</span><span class="params">(s)</span>:</span></div><div class="line">    h = <span class="number">0</span></div><div class="line">    n = len(s)</div><div class="line">    <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(s):</div><div class="line">        h = h + ord(c)*<span class="number">31</span>**(n<span class="number">-1</span>-i)</div><div class="line">    <span class="keyword">return</span> convert_4_bytes(h)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">hashCode = getHashCode(line.strip())</div><div class="line"></div><div class="line"><span class="keyword">print</span>  (hashCode &amp; <span class="number">2147483647</span>) % reduceNum</div></pre></td></tr></table></figure><blockquote><p>经过以上的代码验证得出这些key确实是会被放到同一个partition中,后来了解到同事用的是上一个mapreduce计算逻辑的输出中的一个part作为输入,所以也就可以捋顺之前为什么会集中到reduce输出结果中的某一个part中而不分散的缘故.</p></blockquote><h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><blockquote><p>由此可见这样的方法,我们可以预估到这些key是不是交给同一个reduce去处理,但是并不能准确定位到它会出现在reduce输出结果的那个part中,只会是按照排序之后依次向下排序.所以如果所有的key最终如果只会交给一个reduce处理它的结果就只会出现在part0000中.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景描述&quot;&gt;&lt;a href=&quot;#背景描述&quot; class=&quot;headerlink&quot; title=&quot;背景描述&quot;&gt;&lt;/a&gt;背景描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;17G的输入数据经过map处理输出,map中只有打标记的处理,然后reduce只有cat操作,但是结果这些结果集中到一个part中&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>spring 优雅地关闭程序</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/27/spring-%E4%BC%98%E9%9B%85%E5%9C%B0%E5%85%B3%E9%97%AD%E7%A8%8B%E5%BA%8F/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/27/spring-优雅地关闭程序/</id>
    <published>2018-04-27T09:11:06.000Z</published>
    <updated>2018-04-27T09:15:16.722Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><blockquote><p>如何优雅地退出spring容器环境?</p></blockquote><a id="more"></a><h2 id="细节描述"><a href="#细节描述" class="headerlink" title="细节描述"></a>细节描述</h2><h3 id="Spring关闭钩子"><a href="#Spring关闭钩子" class="headerlink" title="Spring关闭钩子"></a>Spring关闭钩子</h3><blockquote><p>Spring在AbstractApplicationContext里维护了一个shutdownHook属性，用来关闭Spring上下文。但这个钩子不是默认生效的，需要手动调用&gt;ApplicationContext.registerShutdownHook()来开启，在自行维护ApplicationContext（而不是托管给tomcat之类的容器时），注意尽量使用&gt;ApplicationContext.registerShutdownHook()或者手动调用ApplicationContext.close()来关闭Spring上下文，否则应用退出时可能会残留资源。</p></blockquote><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><blockquote><p>Runtime.getRuntime().addShutdownHook的方法的意思就是在jvm中增加一个关闭的钩子，当jvm关闭的时候，会执行系统中已设置的所有通过addShutdownHook添加的钩子，当系统执行完这些钩子后，jvm才会关闭。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;如何优雅地退出spring容器环境?&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hadoop HDFS 数据自动平衡</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/27/%20Hadoop%20HDFS%20%E6%95%B0%E6%8D%AE%E8%87%AA%E5%8A%A8%E5%B9%B3%E8%A1%A1/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/27/ Hadoop HDFS 数据自动平衡/</id>
    <published>2018-04-27T05:53:42.269Z</published>
    <updated>2018-04-27T05:53:42.269Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Hadoop-HDFS-数据自动平衡脚本使用方法"><a href="#Hadoop-HDFS-数据自动平衡脚本使用方法" class="headerlink" title="Hadoop HDFS 数据自动平衡脚本使用方法"></a>Hadoop HDFS 数据自动平衡脚本使用方法</h2><blockquote><p>在Hadoop中，包含一个start-balancer.sh脚本，通过运行这个工具，启动HDFS数据均衡服务。该工具可以做到热插拔，即无须重启计算机和 Hadoop 服务。HadoopHome/bin目录下的start−balancer.sh脚本就是该任务的启动脚本。启动命令为：‘HadoopHome/bin目录下的start−balancer.sh脚本就是该任务的启动脚本。启动命令为：‘Hadoop_home/bin/start-balancer.sh –threshold`<br><a id="more"></a></p></blockquote><h2 id="影响Balancer的几个参数："><a href="#影响Balancer的几个参数：" class="headerlink" title="影响Balancer的几个参数："></a>影响Balancer的几个参数：</h2><p>-threshold</p><blockquote><p>默认设置：10，参数取值范围：0-100<br>参数含义：判断集群是否平衡的阈值。理论上，该参数设置的越小，整个集群就越平衡<br>dfs.balance.bandwidthPerSec<br>默认设置：1048576（1M/S）<br>参数含义：Balancer运行时允许占用的带宽<br>示例如下：</p></blockquote><p>#启动数据均衡，默认阈值为 10%<br>$Hadoop_home/bin/start-balancer.sh</p><p>#启动数据均衡，阈值 5%<br>bin/start-balancer.sh –threshold 5</p><p>#停止数据均衡<br>$Hadoop_home/bin/stop-balancer.sh<br>在hdfs-site.xml文件中可以设置数据均衡占用的网络带宽限制</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.balance.bandwidthPerSec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1048576<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span> Specifies the maximum bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second. <span class="tag">&lt;/<span class="name">description</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Hadoop-HDFS-数据自动平衡脚本使用方法&quot;&gt;&lt;a href=&quot;#Hadoop-HDFS-数据自动平衡脚本使用方法&quot; class=&quot;headerlink&quot; title=&quot;Hadoop HDFS 数据自动平衡脚本使用方法&quot;&gt;&lt;/a&gt;Hadoop HDFS 数据自动平衡脚本使用方法&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;在Hadoop中，包含一个start-balancer.sh脚本，通过运行这个工具，启动HDFS数据均衡服务。该工具可以做到热插拔，即无须重启计算机和 Hadoop 服务。HadoopHome/bin目录下的start−balancer.sh脚本就是该任务的启动脚本。启动命令为：‘HadoopHome/bin目录下的start−balancer.sh脚本就是该任务的启动脚本。启动命令为：‘Hadoop_home/bin/start-balancer.sh –threshold`&lt;br&gt;
    
    </summary>
    
    
      <category term="hadoop" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hadoop/"/>
    
      <category term="hdfs" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hdfs/"/>
    
      <category term="balancer" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/balancer/"/>
    
  </entry>
  
  <entry>
    <title>python utc时间转换为本地时间</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/27/python-utc%E6%97%B6%E9%97%B4%E8%BD%AC%E6%8D%A2%E4%B8%BA%E6%9C%AC%E5%9C%B0%E6%97%B6%E9%97%B4/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/27/python-utc时间转换为本地时间/</id>
    <published>2018-04-27T02:13:35.000Z</published>
    <updated>2018-05-02T01:30:08.350Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景描述"><a href="#背景描述" class="headerlink" title="背景描述"></a>背景描述</h2><blockquote><p>公司业务mongo库中,存了之前很多日志,但是日志中时间没有预处理,也就是说用的是默认的utc时间,我们现实中用到的东八区的时间,公司报表系统要用到这个业务字段,导出日志的时候用的是mongoexport,后续要对时间做处理,想到了用python脚本来二次处理.</p></blockquote><a id="more"></a><h2 id="技术实现"><a href="#技术实现" class="headerlink" title="技术实现"></a>技术实现</h2><blockquote><p>因为mongo导出的时间都是字符串,所以预先要考虑的是字符串格式的数据转换为时间格式的,然后对该时间加八小时格式化输出即可.</p></blockquote><h2 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节:"></a>技术细节:</h2><blockquote><p>先贴代码为敬:</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> dateutil <span class="keyword">import</span> parser</div><div class="line"><span class="keyword">import</span> datetime</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">details = line.strip().split(<span class="string">","</span>)</div><div class="line">time_string = details[<span class="number">0</span>]</div><div class="line">datetime_struct = parser.parse(time_string)</div><div class="line"><span class="comment"># print type(datetime_struct)</span></div><div class="line">o = datetime.timedelta(hours=<span class="number">8</span>)</div><div class="line">details[<span class="number">0</span>] = (datetime_struct+o).strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">"\t"</span>.join(details)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">main()</div></pre></td></tr></table></figure><blockquote><p>字符串时间转为时间格式用到的是dateutil里面parser,是不是很像java里面的?这个模块不是Python自带,我们需要使用pip安装名称如下</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo pip install python-dateutil</div></pre></td></tr></table></figure><blockquote><p>然后使用datetime中的timedelta函数格式化八小时增量模块然后与之前的格式化的时间做相加操作,得到最终要的东八区时间.</p></blockquote><h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><blockquote><p>如何打印出从某个日期往后顺延指定个数的日期,枚举出来.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> datetime</div><div class="line">start = datetime.datetime(<span class="number">2017</span>,<span class="number">10</span>,<span class="number">15</span>)</div><div class="line">zone = datetime.timedelta(days=<span class="number">1</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">184</span>):</div><div class="line">    day = start + zone*i</div><div class="line">    <span class="keyword">print</span> datetime.datetime.strftime(day,<span class="string">"%Y-%m-%d"</span>)</div></pre></td></tr></table></figure><blockquote><p>思路:推算之前大概预估下你要统计多久的顺延的日子,然后使用timedelta以此累加,遍历打印就是最终想要的结果.</p><p>ps:以此类推:某个特定小时,某个特定的月份,往后顺延,大同小异.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景描述&quot;&gt;&lt;a href=&quot;#背景描述&quot; class=&quot;headerlink&quot; title=&quot;背景描述&quot;&gt;&lt;/a&gt;背景描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;公司业务mongo库中,存了之前很多日志,但是日志中时间没有预处理,也就是说用的是默认的utc时间,我们现实中用到的东八区的时间,公司报表系统要用到这个业务字段,导出日志的时候用的是mongoexport,后续要对时间做处理,想到了用python脚本来二次处理.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/python/"/>
    
      <category term="utc" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/utc/"/>
    
      <category term="dateutil" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/dateutil/"/>
    
  </entry>
  
  <entry>
    <title>hexo yilla 和github 结合搭建个人博客</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/16/hexo%20yilia%20github%20%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/16/hexo yilia github 搭建个人博客/</id>
    <published>2018-04-16T03:17:28.107Z</published>
    <updated>2018-04-16T03:17:28.083Z</updated>
    
    <content type="html"><![CDATA[<h1 id="为什么考虑这样的搭配方式"><a href="#为什么考虑这样的搭配方式" class="headerlink" title="为什么考虑这样的搭配方式?"></a>为什么考虑这样的搭配方式?</h1><h2 id="构建需求"><a href="#构建需求" class="headerlink" title="构建需求"></a>构建需求</h2><blockquote><p>现阶段有很多的技术网站都带有给想要展示自己的一些技术入门以及技术研究的平台,也就常见的技术博客.<br>  但是大多都不满足于个性化定制,偶然间接触到markdown,当然简书等这样的平台也支持markdown,但是出于个人的独占情节,还是更倾向于搭建一个独立的自己可控的blog.<br><a id="more"></a></p><h2 id="技术实现-快速搭建"><a href="#技术实现-快速搭建" class="headerlink" title="技术实现(快速搭建)"></a>技术实现(快速搭建)</h2><p>考虑到如果从零开始,买空间,选域名,构建主体框架,渲染静态页面,一套走下来,未免本末倒置,博客注重的应该是文章的可读性以及质量,当然ui需要一定的可观瞻性.幸运的是遇到了hexo,给人一种转角遇到爱的小确幸.<br>      ps:Hexo is a fast, simple &amp; powerful blog framework powered by Node.js.<br>      从官网的解释可以看出,我们需要安装Node.js,当然要和gitbub结合,你需要申请一个github账号,申请账号的步骤,此处就不再累述.<br>      笔者使用的系统是ubantu 16.04</p><p>传送门:<a href="https://github.com/join" target="_blank" rel="external">github账号申请</a></p></blockquote><h3 id="现在演示安装Node-js"><a href="#现在演示安装Node-js" class="headerlink" title="现在演示安装Node.js."></a>现在演示安装Node.js.</h3><blockquote><p>在 Github 上获取 Node.js 源码：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo git <span class="built_in">clone</span> https://github.com/nodejs/node.git</div></pre></td></tr></table></figure><p> 1.修改目录权限：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo chmod -R 755 node</div><div class="line">$ <span class="built_in">cd</span> node</div><div class="line">$ node -v</div></pre></td></tr></table></figure></p><p> 2.使用 ./configure 创建编译文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo ./configure</div></pre></td></tr></table></figure></p><p> 3.这一步，可能时间有点长，耐心等待<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo make</div></pre></td></tr></table></figure></p><p> 4.最后<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo make</div></pre></td></tr></table></figure></p><blockquote><p>install 查看版本</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ node -v</div></pre></td></tr></table></figure><blockquote><p>v0.10.25 如果node不是最新的，node有一个模块叫n，是专门用来管理node.js的版本的。使用npm（NPM是随同nodejs一起安装的包管理工具）安装n模块</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo npm install -g n</div></pre></td></tr></table></figure><blockquote><p>然后，升级node.js到最新稳定版</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo n stable</div></pre></td></tr></table></figure><blockquote><p>旧版本的 npm，也可以很容易地通过 npm 命令来升级，命令如下：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo npm install npm -g</div></pre></td></tr></table></figure><h3 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h3><blockquote><p>执行以下的命令:</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-cli -g</div><div class="line">$ hexo init blog 此处会新建一个新的目录存储hexo的一些初始化的文件.</div><div class="line">$ <span class="built_in">cd</span> blog</div><div class="line">$ npm install</div><div class="line">$ hexo server 此处会生成一个新的本地预览 访问http://localhost:4000 就可以访问本地的默认主题.</div></pre></td></tr></table></figure><h3 id="新建github仓库"><a href="#新建github仓库" class="headerlink" title="新建github仓库"></a>新建github仓库</h3><blockquote><p>新建一个仓库,然后选择public权限(写博客不就是为了别人看,进而监督自己进步么,所以public),指定git分支,使用默认的master分支即可,因为这个仓库就你一个看门的,这里面也是你的.然后记住自己的clone地址.<br>ps:记得保存.</p></blockquote><h3 id="将gitbub仓库和hexo主题绑定"><a href="#将gitbub仓库和hexo主题绑定" class="headerlink" title="将gitbub仓库和hexo主题绑定"></a>将gitbub仓库和hexo主题绑定</h3><blockquote><p>编辑_config.yml:</p></blockquote><figure class="highlight less"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="attribute">type</span>: git</div><div class="line"><span class="attribute">repo</span>: <span class="attribute">https</span>:<span class="comment">//github.com/tangyuxiaoyao/ITWO.git</span></div><div class="line"><span class="attribute">branch</span>: master</div></pre></td></tr></table></figure><blockquote><p>repo配置的地址为上文已经提及过的项目仓库clone地址.<br>而且这里有看到仓库后面带有子资源路径所以参考配置文件中的注释,需要将root对应的配置改为仓库的名称资源路径.</p></blockquote><figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># URL</div><div class="line">## If your site <span class="keyword">is</span> <span class="keyword">put</span> in <span class="keyword">a</span> subdirectory, <span class="keyword">set</span> url <span class="keyword">as</span> <span class="string">'http://yoursite.com/child'</span> <span class="built_in">and</span> root <span class="keyword">as</span> <span class="string">'/child/'</span></div><div class="line">roo<span class="variable">t:</span> /ITWO/</div></pre></td></tr></table></figure><h3 id="发布主题到github仓库"><a href="#发布主题到github仓库" class="headerlink" title="发布主题到github仓库"></a>发布主题到github仓库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">npm install hexo-deployer-git --save</div><div class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</div></pre></td></tr></table></figure><ol><li>安装hexo发布模块deploy</li><li>清除缓存</li><li>生成静态页面</li><li>发布到github(每次改完以后也是这么稳妥发布)<h3 id="生成新的文章"><a href="#生成新的文章" class="headerlink" title="生成新的文章"></a>生成新的文章</h3></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> <span class="built_in">source</span>/_posts/</div><div class="line">$ hexo new <span class="string">"shell在指定行插入文本"</span></div></pre></td></tr></table></figure><blockquote><p>然后就会生成一个为该名称的md文件,根据md语法编辑内容,完成以后发布即可.<br>ps:可以用hexo clean &amp;&amp; hexo g &amp;&amp; hexo s在本地生成预览效果,避免发布到github上的效果不尽人意.</p></blockquote><p><img src="/ITWO/assets/jscy.jpg" alt="古人笑比庭中树,一日秋风一日疏"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;为什么考虑这样的搭配方式&quot;&gt;&lt;a href=&quot;#为什么考虑这样的搭配方式&quot; class=&quot;headerlink&quot; title=&quot;为什么考虑这样的搭配方式?&quot;&gt;&lt;/a&gt;为什么考虑这样的搭配方式?&lt;/h1&gt;&lt;h2 id=&quot;构建需求&quot;&gt;&lt;a href=&quot;#构建需求&quot; class=&quot;headerlink&quot; title=&quot;构建需求&quot;&gt;&lt;/a&gt;构建需求&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;现阶段有很多的技术网站都带有给想要展示自己的一些技术入门以及技术研究的平台,也就常见的技术博客.&lt;br&gt;  但是大多都不满足于个性化定制,偶然间接触到markdown,当然简书等这样的平台也支持markdown,但是出于个人的独占情节,还是更倾向于搭建一个独立的自己可控的blog.&lt;br&gt;
    
    </summary>
    
    
      <category term="hexo" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hexo/"/>
    
      <category term="yilla" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/yilla/"/>
    
      <category term="github" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>如何在hadoop streaming mapreduce中 使用python第三方包</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/11/%E5%A6%82%E4%BD%95%E5%9C%A8hadoop-streaming-mapreduce%E4%B8%AD-%E4%BD%BF%E7%94%A8python%E7%AC%AC%E4%B8%89%E6%96%B9%E5%8C%85/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/11/如何在hadoop-streaming-mapreduce中-使用python第三方包/</id>
    <published>2018-04-11T09:26:09.000Z</published>
    <updated>2018-04-16T02:43:46.578Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景描述"><a href="#背景描述" class="headerlink" title="背景描述"></a>背景描述</h2><blockquote><p>当我们需要在大数据平台使用python第三方包的时候，第一印象是在集群的每台集群都安装该第三方Python包，但是这样需要集群重启，很明显此方案扑街。<br><a id="more"></a></p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>我们使用hadoop streaming提供你的压缩文件分发命令来使用我们的第三方Python包。</p></blockquote><h2 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h2><blockquote><p>我们先了解下该命令：</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">-archivesOptionalSpecify comma-separated archives to be unarchived on the compute machines</div></pre></td></tr></table></figure><blockquote><p>字面意思是上传多个以逗号分割的压缩包到集群然后会分发解压到各个计算节点。</p></blockquote><h3 id="打包"><a href="#打包" class="headerlink" title="打包"></a>打包</h3><blockquote><p>打包集成python和第三方python包。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> 需要使用第三方库如bs4,numpy等时，需要用到虚拟环境virtualenv</span></div><div class="line"></div><div class="line"><span class="meta">#</span><span class="bash"> virtualenv的使用</span></div><div class="line"><span class="meta">#</span><span class="bash"> 安装</span></div><div class="line"></div><div class="line">pip install virtualenv</div><div class="line"><span class="meta">#</span><span class="bash"> 新建虚拟环境</span></div><div class="line"></div><div class="line">virtualenv kxvp</div><div class="line"></div><div class="line"><span class="meta">#</span><span class="bash"> 使得虚拟环境的路径为相对路径</span></div><div class="line"></div><div class="line">virtualenv --relocatable kxvp</div><div class="line"><span class="meta">#</span><span class="bash"> 激活虚拟环境</span></div><div class="line"></div><div class="line">source kxvp/bin/activate</div><div class="line"><span class="meta">#</span><span class="bash"> 如果想退出，可以使用下面的命令</span></div><div class="line"></div><div class="line">deactivate</div><div class="line"><span class="meta">#</span><span class="bash"> 激活后直接安装各种需要的包</span></div><div class="line"></div><div class="line"><span class="meta">#</span><span class="bash"> pip install XXX</span></div><div class="line"><span class="meta">#</span><span class="bash"> 压缩环境包</span></div><div class="line"></div><div class="line">tar -czf kxvp.tar.gz kxvp</div></pre></td></tr></table></figure><blockquote><p>上传到集群客户端</p></blockquote><h3 id="编写测试脚本"><a href="#编写测试脚本" class="headerlink" title="编写测试脚本"></a>编写测试脚本</h3><blockquote><p>test.sh</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></div><div class="line"></div><div class="line">hadoop fs -rmr $&#123;2&#125;</div><div class="line"></div><div class="line">hadoop jar /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/hadoop-streaming-2.6.0-cdh5.4.0.jar \</div><div class="line">        -input $&#123;1&#125;\</div><div class="line">        -output $&#123;2&#125; \</div><div class="line">        -file map.py \</div><div class="line">        -mapper "kx/kxvp/bin/python map.py" \</div><div class="line">-cacheArchive '/user/upsmart/koulb/kxvp.tar.gz#kx'\</div><div class="line">        -jobconf mapred.reduce.tasks=0 \</div><div class="line">        -jobconf mapred.job.name="cache_archive_test"</div></pre></td></tr></table></figure><blockquote><p>map.py</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(separator = <span class="string">','</span>)</span>:</span></div><div class="line"><span class="keyword">print</span> np.__version__</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure><blockquote><p>tip:Python的执行路径注意要加上#后面的前缀</p></blockquote><h3 id="结果验证"><a href="#结果验证" class="headerlink" title="结果验证"></a>结果验证</h3><blockquote><p>看下集群客户端的numpy版本</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">upsmart@cluster-8:~/test_tmp$ python</div><div class="line">Python 2.7.6 (default, Oct 26 2016, 20:30:19) </div><div class="line">[GCC 4.8.4] on linux2</div><div class="line">Type "help", "copyright", "credits" or "license" for more information.</div><div class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import numpy</span></div><div class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="built_in">print</span> numpy.__version__</span></div><div class="line">1.8.2</div></pre></td></tr></table></figure><blockquote><p>看下mapreduce输出的结果</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">upsmart@cluster-8:~/test_tmp$ hadoop fs -getmerge koulb/test/p* rs</div><div class="line">upsmart@cluster-8:~/test_tmp$ more rs</div><div class="line">1.14.2</div></pre></td></tr></table></figure><blockquote><p>不一样验证通过.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景描述&quot;&gt;&lt;a href=&quot;#背景描述&quot; class=&quot;headerlink&quot; title=&quot;背景描述&quot;&gt;&lt;/a&gt;背景描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;当我们需要在大数据平台使用python第三方包的时候，第一印象是在集群的每台集群都安装该第三方Python包，但是这样需要集群重启，很明显此方案扑街。&lt;br&gt;
    
    </summary>
    
    
      <category term="hadoop" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hadoop/"/>
    
      <category term="streaming" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/streaming/"/>
    
      <category term="python" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/python/"/>
    
      <category term="mapreduce" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/mapreduce/"/>
    
      <category term="numpy" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>shell 遍历多级目录取文件的第一行</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/08/shell-%E9%81%8D%E5%8E%86%E5%A4%9A%E7%BA%A7%E7%9B%AE%E5%BD%95%E5%8F%96%E6%96%87%E4%BB%B6%E7%9A%84%E7%AC%AC%E4%B8%80%E8%A1%8C/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/08/shell-遍历多级目录取文件的第一行/</id>
    <published>2018-04-08T11:45:10.000Z</published>
    <updated>2018-04-16T02:44:03.373Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><blockquote><p>数据取样，从当前目录下去遍历多级目录拿到每个文件的第一行数据输出到一个文件中。<br><a id="more"></a></p></blockquote><h2 id="技术分析"><a href="#技术分析" class="headerlink" title="技术分析"></a>技术分析</h2><blockquote><p>当听到该需求时，注意点有两点</p></blockquote><ol><li>多级目录</li><li>取文件的第一行</li></ol><blockquote><p>for循环去递归遍历</p></blockquote><h2 id="技术实现"><a href="#技术实现" class="headerlink" title="技术实现"></a>技术实现</h2><blockquote><p>viewFile.sh<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></div><div class="line">function getdir()&#123;</div><div class="line">    for element in `ls $1`</div><div class="line">    do  </div><div class="line">        dir_or_file=$1"/"$element</div><div class="line">        if [ -d $dir_or_file ]</div><div class="line">        then </div><div class="line">            getdir $dir_or_file</div><div class="line">        else</div><div class="line">           head -1 $dir_or_file</div><div class="line">        fi  </div><div class="line">    done</div><div class="line">&#125;</div><div class="line">root_dir="/home/test/multiFolder"</div><div class="line">getdir $root_dir</div></pre></td></tr></table></figure></p><p>代码如上请笑纳。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求背景&quot;&gt;&lt;a href=&quot;#需求背景&quot; class=&quot;headerlink&quot; title=&quot;需求背景&quot;&gt;&lt;/a&gt;需求背景&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;数据取样，从当前目录下去遍历多级目录拿到每个文件的第一行数据输出到一个文件中。&lt;br&gt;
    
    </summary>
    
    
      <category term="shell" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/shell/"/>
    
      <category term="file" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/file/"/>
    
  </entry>
  
  <entry>
    <title>mysql 和mongo 导入数据总结</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/04/04/mysql-%E5%92%8Cmongo-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E6%80%BB%E7%BB%93/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/04/04/mysql-和mongo-导入数据总结/</id>
    <published>2018-04-04T07:17:22.000Z</published>
    <updated>2018-04-16T02:43:57.221Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><blockquote><p>线上系统环境迁移，数据部分mysql和mongo需要迁移，大部分的数据通过dump，已经迁移过去，迁移过程中会有增量的数据产生，这部分数据要怎么做到无缝迁移。<br><a id="more"></a></p><h2 id="实现分析"><a href="#实现分析" class="headerlink" title="实现分析"></a>实现分析</h2><p>因为系统落地之前都会放到队列里面等待消费，然后落库，所以新系统保证服务正常以后，域名解析到新环境，然后流程环节知道队列，等待老版环境队列消费完了而且ngnix中无用户请求记录进来（因为域名解析会有几分钟的时间），我们开始提取增量的数据，mysql记录的是表记录的最大id，mongo记录也是最大的集合objectId，然后批量导出。</p></blockquote><h2 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h2><h3 id="mysql"><a href="#mysql" class="headerlink" title="mysql"></a>mysql</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">导出</span></div><div class="line">mysql -uupsmart -p dbname -e 'select * from t where id&gt;max(id)' &gt;new_data.tsv</div><div class="line"><span class="meta">#</span><span class="bash">导入</span></div><div class="line">mysqlimport [options] db_name textfile1 ...</div></pre></td></tr></table></figure><blockquote><p>tip①:max(id)为dump之前记录的最大id。<br>tip②:mysqllimport导入的时候一定要指定-local，当然这部分属于options，或者写的是绝对路径否则会报错，默认找的是和mysqlimport命令同一路径下面的文件<br>tip③:dbname紧贴要导入的文件，而且textfile1的前缀名即为表名（.之前的那货）</p></blockquote><h3 id="mongo"><a href="#mongo" class="headerlink" title="mongo"></a>mongo</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">导出</span></div><div class="line">mongoexport -d dbanme -c collname --type=json -o new_data.csv --query='&#123;"_id":"&#123;"$gt":ObjectId("max(id)")&#125;"&#125;' </div><div class="line"><span class="meta">#</span><span class="bash">导入</span></div><div class="line">mongoimport  -d dbanme -c collname  --type=json --file new_data.csv</div></pre></td></tr></table></figure><blockquote><p>tip:本来打算是导出的时候指定csv格式然后导出，但是使用mongexport制定–type=csv的时候不能导出全部的字段，后来选择了默认的json格式导出。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求背景&quot;&gt;&lt;a href=&quot;#需求背景&quot; class=&quot;headerlink&quot; title=&quot;需求背景&quot;&gt;&lt;/a&gt;需求背景&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;线上系统环境迁移，数据部分mysql和mongo需要迁移，大部分的数据通过dump，已经迁移过去，迁移过程中会有增量的数据产生，这部分数据要怎么做到无缝迁移。&lt;br&gt;
    
    </summary>
    
    
      <category term="mongo" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/mongo/"/>
    
      <category term="mysql" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>清洗流水之去除字段中多余的分隔符</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/03/30/%E6%B8%85%E6%B4%97%E6%B5%81%E6%B0%B4%E4%B9%8B%E5%8E%BB%E9%99%A4%E5%AD%97%E6%AE%B5%E4%B8%AD%E5%A4%9A%E4%BD%99%E7%9A%84%E5%88%86%E9%9A%94%E7%AC%A6/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/03/30/清洗流水之去除字段中多余的分隔符/</id>
    <published>2018-03-30T06:48:57.000Z</published>
    <updated>2018-04-24T08:05:46.797Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><blockquote><p>正常的公司流水是47个字段，在做ETL的时候发现里面用分隔符去切割流水处理发现有超过47长度的，毋庸置疑肯定是其中有个别字段中含有分隔符，后来定位到是商户地址中含有分隔符而且不规则会出现多个，现在需要做的是将商户名称中的分隔符替换为空，然后重新拼装成我们需要的流水输出。<br><a id="more"></a></p></blockquote><h2 id="技术实现"><a href="#技术实现" class="headerlink" title="技术实现"></a>技术实现</h2><blockquote><p>直接贴代码</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeDirtyData</span><span class="params">(sep=<span class="string">","</span>)</span>:</span></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">details = line.strip().split(sep,<span class="number">39</span>)</div><div class="line">bad_col = details[<span class="number">39</span>].split(sep)[:<span class="number">-7</span>]</div><div class="line"><span class="keyword">if</span>(len(bad_col)&gt;<span class="number">1</span>):</div><div class="line">fix_col = <span class="string">""</span>.join(bad_col)</div><div class="line"><span class="keyword">print</span> <span class="string">"%s,%s,%s"</span> % (sep.join(details[:<span class="number">39</span>]),fix_col,sep.join(details[<span class="number">39</span>].split(sep)[<span class="number">-7</span>:]))</div><div class="line"><span class="keyword">else</span>:</div><div class="line"><span class="keyword">print</span>  line.strip()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">removeDirtyData()</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">main()</div></pre></td></tr></table></figure><h2 id="细节分析"><a href="#细节分析" class="headerlink" title="细节分析"></a>细节分析</h2><ol><li>先定位到需要的截取字段之前的位置切割，然后把后面的部分单独处理。</li><li>后面部分的数据使用[:-num]的方式，拿到脏数据中的异常字段数组</li><li>通过join的特性用空替换掉里面的分隔符</li><li>分三部分把清洗好的流水拼接起来。</li><li>当然正常的数据不用处理正常输出。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求描述&quot;&gt;&lt;a href=&quot;#需求描述&quot; class=&quot;headerlink&quot; title=&quot;需求描述&quot;&gt;&lt;/a&gt;需求描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;正常的公司流水是47个字段，在做ETL的时候发现里面用分隔符去切割流水处理发现有超过47长度的，毋庸置疑肯定是其中有个别字段中含有分隔符，后来定位到是商户地址中含有分隔符而且不规则会出现多个，现在需要做的是将商户名称中的分隔符替换为空，然后重新拼装成我们需要的流水输出。&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/python/"/>
    
      <category term="join" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/join/"/>
    
      <category term="split" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/split/"/>
    
  </entry>
  
  <entry>
    <title>巧用hbase shell 批量提取数据</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/03/28/%E5%B7%A7%E7%94%A8hbase-shell-%E6%89%B9%E9%87%8F%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/03/28/巧用hbase-shell-批量提取数据/</id>
    <published>2018-03-28T06:47:09.000Z</published>
    <updated>2018-03-28T07:44:56.571Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h2><blockquote><p>有一批卡号的需求提取全量的指标数据，这批数据存在于hbase中，所以要考虑去做mapping，来拿到数据，首先考虑的是通过hive sql语句去做join拿到要的数据，当然也是最简单，但是该hbase集群没有装hive，就是这么意外，第二策略是使用Python访问hbase(happybase),但是也要安装很多东西，最后是使用hbase shell 拿到数据以后，再批量拼装。<br><a id="more"></a></p><h2 id="技术分析实现"><a href="#技术分析实现" class="headerlink" title="技术分析实现"></a>技术分析实现</h2></blockquote><h3 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> hive_hbase_test(<span class="keyword">key</span> <span class="built_in">int</span>,<span class="keyword">value</span> <span class="keyword">string</span>)</div><div class="line"><span class="keyword">stored</span> <span class="keyword">by</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span> <span class="keyword">with</span> serdeproperties(<span class="string">"hbase.columns.mapping"</span>=<span class="string">":key,cf1:val"</span>) tblproperties(<span class="string">"hbase.table.name"</span>=<span class="string">"hive_hbase_test"</span>);</div></pre></td></tr></table></figure><blockquote><p>在现有hbase数据的基础上，我们建立一张外表打通hive和hbase数据的映射关系，然后使用我们熟悉的sqljoin然后 通过 hive -e 导出到本地收工。</p></blockquote><h3 id="python"><a href="#python" class="headerlink" title="python"></a>python</h3><blockquote><p>此种方法没有试过，只是存在与理论中,可以参考此篇文章，优势:可以使用hbase连接池（因为是通过停thift协议通信，tcp通信）。<br><a href="https://my.oschina.net/wolfoxliu/blog/856175" target="_blank" rel="external">Hbase实战教程之happybase</a></p></blockquote><h3 id="hbase，shell-，python"><a href="#hbase，shell-，python" class="headerlink" title="hbase，shell ，python"></a>hbase，shell ，python</h3><blockquote><p>次篇幅的重点介绍的也是这种方法。</p></blockquote><ol><li>利用shell生成要提取的hbase shell语句<blockquote><p>利用shell 拼接字符的方式把所有的查询语句生成到一个文件中，当然如果需要查询的key较多你也可以多生成到几个文件中，此处不再赘述，进而使用hbase shell 语句文件得到解析到结果。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">FILENAME=$1</div><div class="line">cat $FILENAME | while read LINE</div><div class="line">do</div><div class="line">echo "exists '$LINE'" &gt;&gt;temp_get</div><div class="line">echo "get 'card_md5','$LINE'" &gt;&gt;temp_get</div><div class="line">done</div><div class="line">hbase shell temp_get &gt;UDcredit-20180124-1_card_res.csv</div><div class="line"></div><div class="line">if test $? -eq 0</div><div class="line">then</div><div class="line">exit</div><div class="line">fi</div></pre></td></tr></table></figure></blockquote></li></ol><blockquote><p>要点:因为hbase shell查询出的结果中不会有key，所以这里我们需要暂存下key到文件中，并且肯定是要伴随着结果的输出，这样便于我们处理。所以shell逻辑如上。</p></blockquote><ol><li>利用Python处理结果文件为我们要的kv形式</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"></div><div class="line">md5_aes=&#123;&#125;</div><div class="line"></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">detail = line.strip()</div><div class="line"><span class="keyword">if</span>(<span class="string">'Table'</span> <span class="keyword">in</span> detail):</div><div class="line">lines = detail.split(<span class="string">" "</span>)</div><div class="line">md5card = lines[<span class="number">1</span>]</div><div class="line">md5_aes[md5card]=<span class="string">''</span></div><div class="line"><span class="keyword">if</span>(<span class="string">"value"</span> <span class="keyword">in</span> detail):</div><div class="line">lines = detail.split(<span class="string">"value="</span>)</div><div class="line">sm3card = lines1</div><div class="line">md5_aes[md5card]=sm3card</div><div class="line"></div><div class="line"><span class="keyword">for</span> key <span class="keyword">in</span> md5_aes.keys():</div><div class="line"><span class="keyword">print</span> key+<span class="string">","</span>+md5_aes[key]</div></pre></td></tr></table></figure><blockquote><p>利用当前行的特殊关键字来分割当前行数据，然后利用字典，拼接kv输出得到我们要的结果。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求描述&quot;&gt;&lt;a href=&quot;#需求描述&quot; class=&quot;headerlink&quot; title=&quot;需求描述&quot;&gt;&lt;/a&gt;需求描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;有一批卡号的需求提取全量的指标数据，这批数据存在于hbase中，所以要考虑去做mapping，来拿到数据，首先考虑的是通过hive sql语句去做join拿到要的数据，当然也是最简单，但是该hbase集群没有装hive，就是这么意外，第二策略是使用Python访问hbase(happybase),但是也要安装很多东西，最后是使用hbase shell 拿到数据以后，再批量拼装。&lt;br&gt;
    
    </summary>
    
    
      <category term="shell" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/shell/"/>
    
      <category term="python" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/python/"/>
    
      <category term="hbase" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>mapreduce 温故</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/03/23/mapredcue%20%E6%B8%A9%E4%B9%A0/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/03/23/mapredcue 温习/</id>
    <published>2018-03-23T03:09:47.374Z</published>
    <updated>2018-03-23T03:09:47.374Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求背景："><a href="#需求背景：" class="headerlink" title="需求背景："></a>需求背景：</h2><blockquote><p>做大数据有一段时间了，梳理下用到mapreduce的一些问题和解决方案。<a id="more"></a></p></blockquote><h2 id="mapreduce"><a href="#mapreduce" class="headerlink" title="mapreduce"></a>mapreduce</h2><blockquote><p>mapreduce:顾名思义，map做映射，reduce做规约。<br>主要分以下步骤：<br>1.输入分块<br>2.map<br>3.shuffer<br>4.reduce</p></blockquote><p><img src="/ITWO/assets/mapreduce01.jpg" alt="mapredcue流程图"><br>重点是shuffer阶段</p><h2 id="reduce个数的计算方法"><a href="#reduce个数的计算方法" class="headerlink" title="reduce个数的计算方法:"></a>reduce个数的计算方法:</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">double</span> bytes = Math.max(totalInputFileSize, bytesPerReducer);</div><div class="line"><span class="keyword">int</span> reducers = (<span class="keyword">int</span>) Math.ceil(bytes / bytesPerReducer);</div><div class="line">reducers = Math.max(<span class="number">1</span>, reducers);</div><div class="line">reducers = Math.min(maxReducers, reducers);</div></pre></td></tr></table></figure><blockquote><p>　从计算逻辑可以看出该量由输入文件的大小以及设置的每个reduce可以处理的字节数大小决定．</p></blockquote><p><img src="/ITWO/assets/mapreduce02.png" alt="shuffer流程图"></p><h2 id="流程细节："><a href="#流程细节：" class="headerlink" title="流程细节："></a>流程细节：</h2><h3 id="map输出过程："><a href="#map输出过程：" class="headerlink" title="map输出过程："></a>map输出过程：</h3><blockquote><p>&ensp;&ensp;&ensp;&ensp;如果没有reduce阶段，则直接输出到hdfs上，如果有reduce作业，则每个map方法的输出在写磁盘前先在内存中缓存。每个map task都有一个环状的内存缓冲区，存储着map的输出结果，默认100m，在写磁盘时，根据reduce的数量把数据划分为相应的分区(使用默认的分区算法（对输入文件的kv中对key hash后再对reduce task数量取模(reduce个数的算法见前文)),默认的hashPartioner只会作用默认分隔符分割以后的key，如果需要自定义分区，则需要你自定义二次分区比如keyfieldParttioner来实现,在每个分区中数据进行内排序，分区的个数和reduce的个数是一致的，在每次当缓冲区快满的时候由一个独立的线程将缓冲区的数据以一个溢出文件的方式存放到磁盘(这个溢写是由单独线程来完成，不影响往缓冲区写map结果的线程。溢写线程启动时不应该阻止map的结果输出，所以整个缓冲区有个溢写的比例spill.percent。这个比例默认是0.8，也就是当缓冲区的数据已经达到阈值（buffer size <em> spill percent = 100MB </em> 0.8 = 80MB），溢写线程启动，锁定这80MB的内存，执行溢写过程。Map task的输出结果还可以往剩下的20MB内存中写，互不影响。)，当整个map task结束后再对磁盘中这个map task产生的所有溢出文件做合并，被合并成已分区且已排序的输出文件。然后reduce开始fetch（拉取）map端合并好对应分区的数据，然后在reduce端合并（因为会有很多map的输出，需要合并），此时在reduce端也会进行一次sort,确保所有map的输出都排序并合并成完成以后，才会启动reduce task,所以怎么才能确保你在reduce逻辑处理时拿到的是你要的排序后的数据配合你的处理就至关重要了。</p></blockquote><h3 id="reducer如何知道要从哪个tasktracker取得map输出呢？"><a href="#reducer如何知道要从哪个tasktracker取得map输出呢？" class="headerlink" title="reducer如何知道要从哪个tasktracker取得map输出呢？"></a>reducer如何知道要从哪个tasktracker取得map输出呢？</h3><blockquote><p>&ensp;&ensp;&ensp;&ensp;map任务成功完成以后，他们会通知其父tasktracker状态已更新，然后taskTracker进而通知jobTracker。这些通知在前面的心跳机制中传输。因此，对于指定作业，jobTracker知道map输出和taskTracker之间的映射关系。reducer中的一个线程定期询问jobTracher以便获取map输出的位置,直到它获得所有输出位置。</p></blockquote><h3 id="map和reduce如何合理控制自己的个数？"><a href="#map和reduce如何合理控制自己的个数？" class="headerlink" title="map和reduce如何合理控制自己的个数？"></a>map和reduce如何合理控制自己的个数？</h3><blockquote><p>&ensp;&ensp;&ensp;&ensp;map的个数是由dfs.block.size控制，该配置可以在执行程序之前由参数（见下文）控制，默认配置位于hdfs-site.xml中dfs.block.size控制，1.x的默认配置为64m,2.x的默认配置为128m,</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"> <span class="keyword">long</span> goalSize = totalSize / (numSplits == <span class="number">0</span> ? <span class="number">1</span> : numSplits);</div><div class="line"> <span class="keyword">long</span> minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.</div><div class="line">    FileInputFormat.SPLIT_MINSIZE, <span class="number">1</span>), minSplitSize);</div><div class="line"><span class="keyword">long</span> blockSize = file.getBlockSize();</div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">long</span> <span class="title">computeSplitSize</span><span class="params">(<span class="keyword">long</span> goalSize, <span class="keyword">long</span> minSize,</span></span></div><div class="line"><span class="function"><span class="params">                                     <span class="keyword">long</span> blockSize)</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> Math.max(minSize, Math.min(goalSize, blockSize));</div><div class="line">&#125;</div></pre></td></tr></table></figure><blockquote><p>&ensp;&ensp;&ensp;&ensp;从上面可以看出，最终的split size是由三个因素决定，goalsize为map输入数据除以用户自己设置的map个数（默认为1）得到的;minsize为mapred-site.xml配置的mapred.min.split.size决定，因为minSplitSize为1;第三个影响因素为blocksize,这个看配置，最终我们可以得出,如果不设置min.size,则由blocksize决定，如果设置了，则是由这两者中大的一个决定。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="built_in">set</span> mapred.min.split.size=256000000;        -- 决定每个map处理的最大的文件大小，单位为B</div><div class="line"></div><div class="line">方法1</div><div class="line"><span class="built_in">set</span> mapred.reduce.tasks=10;  -- 设置reduce的数量</div><div class="line">方法2</div><div class="line"><span class="built_in">set</span> hive.exec.reducers.bytes.per.reducer=1073741824 -- 每个reduce处理的数据量,默认1GB</div></pre></td></tr></table></figure><p>block_size : hdfs的文件块大小，默认为64M，可以通过参数dfs.block.size设置<br>total_size : 输入文件整体的大小<br>input_file_num : 输入文件的个数</p><p>（1）默认map个数</p><blockquote><p>如果不进行任何设置，默认的map个数是和blcok_size相关的。<br>   default_num = total_size / block_size;</p></blockquote><p>（2）期望大小</p><blockquote><p>可以通过参数mapred.map.tasks来设置程序员期望的map个数，但是这个个数只有在大于default_num的时候，才会生效。<br>   goal_num = mapred.map.tasks;</p></blockquote><p>（3）设置处理的文件大小</p><blockquote><p>可以通过mapred.min.split.size 设置每个task处理的文件大小，但是这个大小只有在大于block_size的时候才会生效。<br>   split_size = max(mapred.min.split.size, block_size);<br>   split_num = total_size / split_size;</p></blockquote><p>（4）计算的map个数</p><blockquote><p>compute_map_num = min(split_num,  max(default_num, goal_num))</p><p>&ensp;&ensp;&ensp;&ensp;除了这些配置以外，mapreduce还要遵循一些原则。 mapreduce的每一个map处理的数据是不能跨越文件的，也就是说min_map_num &gt;= input_file_num。 所以，最终的map个数应该为：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">final_map_num = max(compute_map_num, input_file_num)</div></pre></td></tr></table></figure><blockquote><p>经过以上的分析，在设置map个数的时候，可以简单的总结为以下几点：<br>（1）如果想增加map个数，则设置mapred.map.tasks 为一个较大的值。<br>（2）如果想减小map个数，则设置mapred.min.split.size 为一个较大的值。</p></blockquote><p>reduce个数的设置则相对简单，要么你设置mapred.reduce.tasks的数值，要么你在hive中可以设置每个reduce可以处理的字节数，从而约束reduce的个数。</p><blockquote><p>小技巧</p></blockquote><p>&ensp;&ensp;&ensp;&ensp;在hive中带空的设置参数可以打印出当前该参数的设置值。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hive&gt; set dfs.block.size;</div><div class="line">dfs.block.size=268435456</div><div class="line">hive&gt; set mapred.map.tasks;</div><div class="line">mapred.map.tasks=2</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求背景：&quot;&gt;&lt;a href=&quot;#需求背景：&quot; class=&quot;headerlink&quot; title=&quot;需求背景：&quot;&gt;&lt;/a&gt;需求背景：&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;做大数据有一段时间了，梳理下用到mapreduce的一些问题和解决方案。
    
    </summary>
    
    
      <category term="mapreduce" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/mapreduce/"/>
    
      <category term="shuffer" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/shuffer/"/>
    
  </entry>
  
  <entry>
    <title>nohup &amp; &gt; 使用解释</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/03/14/nohup-%E4%BD%BF%E7%94%A8%E8%A7%A3%E9%87%8A/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/03/14/nohup-使用解释/</id>
    <published>2018-03-14T01:54:12.000Z</published>
    <updated>2018-04-16T03:19:39.333Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景描述"><a href="#背景描述" class="headerlink" title="背景描述"></a>背景描述</h2><blockquote><p>我们在运行服务或者其他程序的时候经常这样启动进程。<a id="more"></a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nohup ls &gt;out.log 2&gt;&amp;1 &amp;</div></pre></td></tr></table></figure><h2 id="细节分析"><a href="#细节分析" class="headerlink" title="细节分析"></a>细节分析</h2><blockquote><p>&amp; 可以免疫SIGINT（sign of interuput），也就是说他可以用于程序后台运行，但是关闭当前shell session就会退出该程序。<br>nohup 的运用是为了免疫SIGHUP信号，可以避免挂起，将程序的输出重定向到nohup.out中，也就是说免疫回话级别的关闭，但是不免疫SIGINT信号，也就是我们所用的Ctrl+C。<br>综上所属我们需要做的就是结合两者的优点，可以这样使用从而真正意义上实现程序后台运行，而且免疫session关闭而导致程序退出。<br>2&gt;&amp;1 的运用是为了将标准出错重定向到标准输出。<br>nohup ls &gt;out.log 2&gt;&amp;1 &amp; 连接起来的意思就是后台挂起ls命令，并且将标准输出和标准出错都作为输出重定向到out.log中，我们可以使用tail -f out.log监控程序的运行过程。<br>tip:当当前session还为关闭，我们可以使用Jobs命令来查看当前会话下面有多少挂起的程序，如果想停止自己启动的程序，可以使用fg %jobnumber的方式，将该任务放到前台执行然后使用Ctrl+C命令终止它的执行。但是如果当前session已经失去，我们就只能通过ps -ef 找到提交的任务然后通过kill来结束程序。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景描述&quot;&gt;&lt;a href=&quot;#背景描述&quot; class=&quot;headerlink&quot; title=&quot;背景描述&quot;&gt;&lt;/a&gt;背景描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;我们在运行服务或者其他程序的时候经常这样启动进程。
    
    </summary>
    
    
      <category term="nohup" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/nohup/"/>
    
  </entry>
  
  <entry>
    <title>maven,gradle 问题解决总结</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/03/13/maven,gradle-%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%80%BB%E7%BB%93/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/03/13/maven,gradle-问题解决总结/</id>
    <published>2018-03-13T01:47:51.000Z</published>
    <updated>2018-03-23T03:09:39.999Z</updated>
    
    <content type="html"><![CDATA[<h2 id="jar包找不到问题"><a href="#jar包找不到问题" class="headerlink" title="jar包找不到问题"></a>jar包找不到问题<a id="more"></a></h2><blockquote><p>遇到此类问题，肯定第一思路是配置正确了，而且去官网私服验证过的确存在的gav，但是还是找不到，此种情况下，你需要做的是把原来的本地仓库下的清理干净，然后重新编译导入一次依赖一般问题就会解决，还有一种<a href="http://mvnrepository.com/" target="_blank" rel="external">官网</a>验证了，的确有但是还是拉不下来，此时在官网下方会给出该包的私服仓库地址，你可以在配置文件中，加入该私服地址，然后还是需要把你本地仓库下的清理干净，然后重新导入一边依赖，依赖包就会拉下来。<br>….未完待续</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;jar包找不到问题&quot;&gt;&lt;a href=&quot;#jar包找不到问题&quot; class=&quot;headerlink&quot; title=&quot;jar包找不到问题&quot;&gt;&lt;/a&gt;jar包找不到问题
    
    </summary>
    
    
      <category term="gradle" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/gradle/"/>
    
      <category term="maven" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/maven/"/>
    
  </entry>
  
  <entry>
    <title>idea 导入gradle项目报错</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/03/13/idea-%E5%AF%BC%E5%85%A5gradle%E9%A1%B9%E7%9B%AE%E6%8A%A5%E9%94%99/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/03/13/idea-导入gradle项目报错/</id>
    <published>2018-03-13T01:17:08.000Z</published>
    <updated>2018-04-16T03:18:58.315Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景描述"><a href="#背景描述" class="headerlink" title="背景描述"></a>背景描述</h2><blockquote><p>在导入一个新的gradle项目进idea时，报错如下：<a id="more"></a></p></blockquote><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Error:com/android/builder/model/AndroidProject : Unsupported major.minor version <span class="number">52.0</span>. Please use JDK <span class="number">8</span> or newer.</div><div class="line">&lt;a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html"&gt;Download JDK 8&lt;/a&gt;&lt;br&gt;&lt;a href="select.jdk"&gt;Select a JDK from the File System&lt;/a&gt;</div></pre></td></tr></table></figure><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><blockquote><p>本来是冲着后面给的错误去解决问题（传统经验害死人），后来该改的都改了，最终还是问题依旧，这才想着从前面开始解决，问了同事，同事也说是gradle的版本问题，其实根本不是，为什么不是呢，贴出官网给的答案</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Gradle runs on all major operating systems and requires only a Java JDK or JRE version 7 or higher to be installed. To check, run java -version:</div><div class="line">$ java -version</div><div class="line">java version <span class="string">"1.8.0_121"</span></div></pre></td></tr></table></figure><blockquote><p>从以上的洋文可以看出，只要是7版本以上就可以，所以就考虑到是不是idea本身的问题，在idea的设置里找了一遭，找到了<strong>它默认开启了Android Support,后来把这项支持点掉</strong>以后，重启idea,问题解决。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景描述&quot;&gt;&lt;a href=&quot;#背景描述&quot; class=&quot;headerlink&quot; title=&quot;背景描述&quot;&gt;&lt;/a&gt;背景描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;在导入一个新的gradle项目进idea时，报错如下：
    
    </summary>
    
    
      <category term="idea" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/idea/"/>
    
      <category term="gradle" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/gradle/"/>
    
      <category term="android" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/android/"/>
    
  </entry>
  
  <entry>
    <title>解决ubantu 谷歌浏览器打不开问题</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/03/07/%E8%A7%A3%E5%86%B3ubantu-%E8%B0%B7%E6%AD%8C%E6%B5%8F%E8%A7%88%E5%99%A8%E6%89%93%E4%B8%8D%E5%BC%80%E9%97%AE%E9%A2%98/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/03/07/解决ubantu-谷歌浏览器打不开问题/</id>
    <published>2018-03-07T06:19:10.000Z</published>
    <updated>2018-04-16T03:26:57.819Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景描述"><a href="#背景描述" class="headerlink" title="背景描述"></a>背景描述</h2><blockquote><p>遇到chrome/chromium启动不了</p></blockquote><a id="more"></a><h2 id="定位问题"><a href="#定位问题" class="headerlink" title="定位问题"></a>定位问题</h2><blockquote><p>在/usr/bin/./chromium-browser启动命令，　查看log输出</p><p>log输出如下：<br>[20049:20083:0302/221800.660699:FATAL:nss_util.cc(631)] NSS_VersionCheck(“3.26”) failed. NSS &gt;= 3.26 is required. Please upgrade to the latest NSS, and if you still get this error, contact your distribution maintainer.<br>已放弃 (核心已转储)</p></blockquote><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install libnss3</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景描述&quot;&gt;&lt;a href=&quot;#背景描述&quot; class=&quot;headerlink&quot; title=&quot;背景描述&quot;&gt;&lt;/a&gt;背景描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;遇到chrome/chromium启动不了&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="ubantu" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/ubantu/"/>
    
      <category term="chrome" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/chrome/"/>
    
      <category term="chromium" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/chromium/"/>
    
      <category term="打不开" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/%E6%89%93%E4%B8%8D%E5%BC%80/"/>
    
  </entry>
  
  <entry>
    <title>shell 合并hdfs文件 分发 并加文件校验和</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/03/06/shell-%E5%90%88%E5%B9%B6hdfs%E6%96%87%E4%BB%B6-%E5%88%86%E5%8F%91-%E5%B9%B6%E5%8A%A0%E6%96%87%E4%BB%B6%E6%A0%A1%E9%AA%8C%E5%92%8C/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/03/06/shell-合并hdfs文件-分发-并加文件校验和/</id>
    <published>2018-03-06T02:40:13.000Z</published>
    <updated>2018-04-16T03:21:29.832Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景描述"><a href="#背景描述" class="headerlink" title="背景描述"></a>背景描述</h2><blockquote><p>提供一批带有城市标识的商户号，然后在流水中匹配到卡号，最终通过设备号和卡号的映射关系，输出每个城市商户商户对应的设备号。<a id="more"></a></p></blockquote><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">MapDevice</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">MidCard</span>(<span class="params">card: <span class="type">String</span>, mid: <span class="type">String</span></span>)</span></div><div class="line"><span class="class"></span></div><div class="line"><span class="class">  <span class="title">case</span> <span class="title">class</span> <span class="title">CardDevice</span>(<span class="params">card: <span class="type">String</span>, device: <span class="type">String</span></span>)</span></div><div class="line"><span class="class"></span></div><div class="line"><span class="class"></span></div><div class="line"><span class="class">  <span class="title">case</span> <span class="title">class</span> <span class="title">CityMid</span>(<span class="params">city: <span class="type">String</span>, mid: <span class="type">String</span></span>)</span></div><div class="line"><span class="class"></span></div><div class="line"><span class="class">  <span class="title">val</span> <span class="title">comma</span> </span>= <span class="string">","</span></div><div class="line">  <span class="keyword">val</span> tab = <span class="string">"\t"</span></div><div class="line"></div><div class="line">  <span class="comment">// 从业务逻辑来看直接判断分割以后的列数更能够过滤掉脏数据，当然空行也被pass了。</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"商户数据抓取设备号"</span>).setMaster(<span class="string">"local[*]"</span>)</div><div class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">    <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment">// .filter(_.matches("\\S+")) 过滤空行</span></div><div class="line">    <span class="comment">// 考虑到只有city和mid的前提下，就会产生重复数据，所以要过滤以后再建表。</span></div><div class="line">    <span class="keyword">import</span> sqlContext.implicits._</div><div class="line">    <span class="keyword">val</span> midFilePath = args(<span class="number">0</span>)</div><div class="line">    <span class="keyword">val</span> cardMids = sc.textFile(midFilePath)</div><div class="line">      .filter(_.trim.split(comma, <span class="number">-1</span>).length == <span class="number">6</span>)</div><div class="line">      .map(line =&gt; &#123;</div><div class="line">        <span class="keyword">val</span> details = line.split(comma, <span class="number">-1</span>)</div><div class="line">        details(<span class="number">0</span>) + comma + details(<span class="number">4</span>)</div><div class="line">      &#125;).distinct().map(line =&gt; &#123;</div><div class="line">      <span class="keyword">val</span> details = line.split(comma, <span class="number">-1</span>)</div><div class="line">      <span class="type">CityMid</span>(details(<span class="number">0</span>), details(<span class="number">1</span>))</div><div class="line">    &#125;).toDF()</div><div class="line"></div><div class="line">    cardMids.registerTempTable(<span class="string">"cityMid"</span>)</div><div class="line"></div><div class="line">    <span class="comment">//    cardMids.show()</span></div><div class="line"></div><div class="line">    <span class="keyword">val</span> transFilePath = args(<span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="keyword">val</span> todayMCDF = sc.newAPIHadoopFile[<span class="type">LongWritable</span>, <span class="type">Text</span>, <span class="type">LzoTextInputFormat</span>](transFilePath)</div><div class="line">      .filter(_._2.toString.split(comma, <span class="number">-1</span>).length == <span class="number">47</span>)</div><div class="line">      .map(_._2.toString.split(comma, <span class="number">-1</span>))</div><div class="line">      .map(mpc =&gt; <span class="type">MidCard</span>(mpc(<span class="number">0</span>), mpc(<span class="number">1</span>)))</div><div class="line">      .toDF()</div><div class="line"></div><div class="line">    todayMCDF.registerTempTable(<span class="string">"midcard"</span>)</div><div class="line"></div><div class="line"></div><div class="line">    <span class="keyword">val</span> cardDevicePath = args(<span class="number">2</span>)</div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment">// .filter(_.trim.length &gt; 0) 过滤空行</span></div><div class="line">    <span class="comment">// _.device.matches("\\S+") 过滤匹配到的设备号为空</span></div><div class="line">    <span class="keyword">val</span> cardDeviceDF = sc.textFile(cardDevicePath)</div><div class="line">      .filter(_.trim.split(tab, <span class="number">-1</span>).length == <span class="number">10</span>)</div><div class="line">      .map(line =&gt; &#123;</div><div class="line">        <span class="keyword">val</span> items = line.split(tab, <span class="number">-1</span>)</div><div class="line">        <span class="keyword">val</span> card = items(<span class="number">0</span>)</div><div class="line">        <span class="keyword">val</span> deviceId = items(<span class="number">3</span>)</div><div class="line">        <span class="type">CardDevice</span>(card, deviceId)</div><div class="line">      &#125;).filter(_.device.matches(<span class="string">"\\S+"</span>)).toDF()</div><div class="line"></div><div class="line"></div><div class="line">    cardDeviceDF.registerTempTable(<span class="string">"carddevice"</span>)</div><div class="line"></div><div class="line">    <span class="comment">//    cardDeviceDF.show(10)</span></div><div class="line"></div><div class="line">    <span class="keyword">val</span> resDF = sqlContext.sql(<span class="string">"select city,device from (select card,city from cityMid c  join cardmid m on c.mid=m.mid ) c join carddevice d on d.card=c.card group by city,device"</span>)</div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment">//    resDF.show(5)</span></div><div class="line">    <span class="keyword">val</span> resSavePath = args(<span class="number">3</span>)</div><div class="line"></div><div class="line"></div><div class="line">    resDF.map(value =&gt; value.mkString(comma)).saveAsTextFile(resSavePath)</div><div class="line"></div><div class="line">  &#125;</div></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="meta">#!/usr/bin/env bash</span></div><div class="line"><span class="built_in">set</span> -x</div><div class="line"></div><div class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -ne 4 ]; <span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"参数个数为<span class="variable">$#</span>个"</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"请按照如下格式输入参数"</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"bash map-device.sh 商户信息配置路径 hardin流水路径 卡号和设备号对应关系路径 结果文件存放路径"</span></div><div class="line">        <span class="built_in">exit</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># koulb/cardmid/onemid koulb/cardmid/testtrans.lzo* koulb/cardmid/card_device_sm3 koulb/cardmid/out</span></div><div class="line"></div><div class="line">midconfPath=<span class="variable">$1</span></div><div class="line">transPath=<span class="variable">$2</span></div><div class="line">cardDevicePath=<span class="variable">$3</span></div><div class="line">resPath=<span class="variable">$4</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">spark-submit \</div><div class="line">--class com.upsmart.MapDevice  \</div><div class="line">--num-executors 100 \</div><div class="line">--executor-memory 6G \</div><div class="line">--executor-cores 4 \</div><div class="line">--driver-memory 1G \</div><div class="line">--conf spark.default.parallelism=1000 \</div><div class="line">--conf spark.storage.memoryFraction=0.5 \</div><div class="line">--conf spark.shuffle.memoryFraction=0.3 \</div><div class="line">--master yarn-client map-device-1.0.jar <span class="variable">$midconfPath</span> <span class="variable">$transPath</span> <span class="variable">$cardDevicePath</span> <span class="variable">$resPath</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> [ $? -ne 0 ];<span class="keyword">then</span></div><div class="line">   <span class="built_in">exit</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line">fileName=`date -d <span class="string">"1 hour ago"</span>  +<span class="string">"%Y%m%d%H"</span>`</div><div class="line"></div><div class="line">fileFullName=<span class="variable">$fileName</span><span class="string">".csv"</span></div><div class="line">hadoop fs -getmerge <span class="variable">$resPath</span> <span class="variable">$fileFullName</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> [ $? -ne 0 ];<span class="keyword">then</span></div><div class="line">   <span class="built_in">exit</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"></div><div class="line">awk -F <span class="string">','</span> <span class="string">'&#123;print $2  &gt;$1"_""'</span><span class="variable">$fileName</span><span class="string">'"".csv"&#125;'</span> <span class="variable">$fileFullName</span></div><div class="line"></div><div class="line">rm <span class="variable">$fileFullName</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> `ls`;</div><div class="line"><span class="keyword">do</span></div><div class="line">fileSuffix=`<span class="built_in">echo</span> <span class="variable">$&#123;i##*_&#125;</span>`</div><div class="line">    <span class="keyword">if</span> [ <span class="variable">$fileSuffix</span> == <span class="variable">$fileFullName</span> ];<span class="keyword">then</span></div><div class="line">    fileSum=`md5sum <span class="variable">$i</span>  |cut -d<span class="string">' '</span> -f1`</div><div class="line">    <span class="built_in">echo</span> <span class="variable">$fileSum</span> &gt;<span class="variable">$i</span><span class="string">".md5"</span></div><div class="line">    <span class="keyword">fi</span></div><div class="line"><span class="keyword">done</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> <span class="built_in">test</span> -e .*.crc</div><div class="line"><span class="keyword">then</span></div><div class="line">    rm -f .*.crc</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure><h2 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h2><h3 id="怎么才算是有效的数据过滤？"><a href="#怎么才算是有效的数据过滤？" class="headerlink" title="怎么才算是有效的数据过滤？"></a>怎么才算是有效的数据过滤？</h3><blockquote><p>原本的思路是过滤掉空行就行，但是后续也会有角标越界的风险存在，从而过滤的最终条件为当前行按照分隔符切割以后，必须是标准列数<br>ps: java和scala都存在这样的bug,如果多行数据的最后一列为空，直接使用spit,得到的列数会少1,当然你去便利该数组，也会少个元素，必须这样使用spilt(str,-1),<br>核心代码：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">_._2.toString.split(comma, -1).length == 47</div></pre></td></tr></table></figure><h3 id="如果匹配到的设备号为空怎么办？"><a href="#如果匹配到的设备号为空怎么办？" class="headerlink" title="如果匹配到的设备号为空怎么办？"></a>如果匹配到的设备号为空怎么办？</h3><blockquote><p>开发之初，没有考虑到该因素，后来有问到数据提供的同事，不排除会存在设备好为空或者空格占位，所以使用正则过滤掉这些无意义的输出<br>代码:filter(_.device.matches(“\S+”))</p></blockquote><h3 id="怎么多列的配置文件的前提下，过滤掉同个城市下会存在重复的mid"><a href="#怎么多列的配置文件的前提下，过滤掉同个城市下会存在重复的mid" class="headerlink" title="怎么多列的配置文件的前提下，过滤掉同个城市下会存在重复的mid"></a>怎么多列的配置文件的前提下，过滤掉同个城市下会存在重复的mid</h3><blockquote><p>不多说，直接贴代码</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 考虑到只有city和mid的前提下，就会产生重复数据，所以要过滤以后再建表。</span></div><div class="line"> <span class="keyword">import</span> sqlContext.implicits._</div><div class="line"> <span class="keyword">val</span> midFilePath = args(<span class="number">0</span>)</div><div class="line"> <span class="keyword">val</span> cardMids = sc.textFile(midFilePath)</div><div class="line">   .filter(_.trim.split(comma, <span class="number">-1</span>).length == <span class="number">6</span>)</div><div class="line">   .map(line =&gt; &#123;</div><div class="line">     <span class="keyword">val</span> details = line.split(comma, <span class="number">-1</span>)</div><div class="line">     details(<span class="number">0</span>) + comma + details(<span class="number">4</span>)</div><div class="line">   &#125;).distinct().map(line =&gt; &#123;</div><div class="line">   <span class="keyword">val</span> details = line.split(comma, <span class="number">-1</span>)</div><div class="line">   <span class="type">CityMid</span>(details(<span class="number">0</span>), details(<span class="number">1</span>))</div><div class="line"> &#125;).toDF()</div></pre></td></tr></table></figure><h3 id="如何在最终结果文件中按文件分发？（shell实现以及理论上的scala实现）"><a href="#如何在最终结果文件中按文件分发？（shell实现以及理论上的scala实现）" class="headerlink" title="如何在最终结果文件中按文件分发？（shell实现以及理论上的scala实现）"></a>如何在最终结果文件中按文件分发？（shell实现以及理论上的scala实现）</h3><h4 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">fileName=`date -d <span class="string">"1 hour ago"</span>  +<span class="string">"%Y%m%d%H"</span>`</div><div class="line">fileFullName=<span class="variable">$fileName</span><span class="string">".csv"</span></div><div class="line">hadoop fs -getmerge <span class="variable">$resPath</span> <span class="variable">$fileFullName</span></div><div class="line">awk -F <span class="string">','</span> <span class="string">'&#123;print $2  &gt;$1"_""'</span><span class="variable">$fileName</span><span class="string">'"".csv"&#125;'</span> <span class="variable">$fileFullName</span></div></pre></td></tr></table></figure><blockquote><p>难点1：按第一列去分发第二列内容，参考本博客的其他文章中的awk分发文件。<br>难点2：在awk中引用变量的值</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">koulb@koulb-ubantu:~$ a=<span class="string">"test"</span></div><div class="line">koulb@koulb-ubantu:~$ awk <span class="string">'BEGIN&#123;print "'</span><span class="variable">$a</span><span class="string">'"&#125;'</span> </div><div class="line"><span class="built_in">test</span></div></pre></td></tr></table></figure><h4 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h4><blockquote><p>最终的文件为多个城市对应设备号的结果，需要做的是，把第一列group by 去重，然后遍历该城市列表拿拿到对应的设备号，生成对应的结果文件。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景描述&quot;&gt;&lt;a href=&quot;#背景描述&quot; class=&quot;headerlink&quot; title=&quot;背景描述&quot;&gt;&lt;/a&gt;背景描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;提供一批带有城市标识的商户号，然后在流水中匹配到卡号，最终通过设备号和卡号的映射关系，输出每个城市商户商户对应的设备号。
    
    </summary>
    
    
      <category term="hadoop" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hadoop/"/>
    
      <category term="hdfs" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hdfs/"/>
    
      <category term="shell" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/shell/"/>
    
      <category term="awk" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/awk/"/>
    
      <category term="file" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/file/"/>
    
      <category term="spark" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/spark/"/>
    
      <category term="scala" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/scala/"/>
    
      <category term="suffix" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/suffix/"/>
    
  </entry>
  
  <entry>
    <title>hive trim() 去除空格遇到的问题</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/02/28/hive-trim-%E5%8E%BB%E9%99%A4%E7%A9%BA%E6%A0%BC%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/02/28/hive-trim-去除空格遇到的问题/</id>
    <published>2018-02-28T05:49:22.000Z</published>
    <updated>2018-04-16T03:18:43.224Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><blockquote><p>在提取数据时，有用到使用商户号和终端号join匹配商户信息，但是返现怎么都匹配不到，以为是这两个号可能前后有空格，所以使用trim()函数处理，但还是没有匹配到结果，后来，vi文件 set list，发现 有一字段后面有^I紧跟其后，才警觉，这丫不是tab么，后来查看源码发现，hive的trim()只对空格感兴趣，所以想起可以使用replace,但是想到通用来讲，但是考虑到使用正则好点，所以又查到hive有正则替换。</p></blockquote><a id="more"></a><h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">hive <span class="number">0.10</span>.0源码</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(Text s)</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (s == <span class="keyword">null</span>) &#123;</div><div class="line"></div><div class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    result.set(StringUtils.strip(s.toString(), <span class="string">" "</span>));</div><div class="line"></div><div class="line">    <span class="keyword">return</span> result;</div><div class="line"></div><div class="line">  &#125;</div></pre></td></tr></table></figure><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">用法如下：</div><div class="line"></div><div class="line">      regexp_replace(yourcolname,'\\s+','')</div><div class="line"></div><div class="line">最后问题得以解决。​</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求背景&quot;&gt;&lt;a href=&quot;#需求背景&quot; class=&quot;headerlink&quot; title=&quot;需求背景&quot;&gt;&lt;/a&gt;需求背景&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;在提取数据时，有用到使用商户号和终端号join匹配商户信息，但是返现怎么都匹配不到，以为是这两个号可能前后有空格，所以使用trim()函数处理，但还是没有匹配到结果，后来，vi文件 set list，发现 有一字段后面有^I紧跟其后，才警觉，这丫不是tab么，后来查看源码发现，hive的trim()只对空格感兴趣，所以想起可以使用replace,但是想到通用来讲，但是考虑到使用正则好点，所以又查到hive有正则替换。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="hive" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hive/"/>
    
      <category term="trim" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/trim/"/>
    
      <category term="空格" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/%E7%A9%BA%E6%A0%BC/"/>
    
      <category term="tab" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/tab/"/>
    
      <category term="制表符" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/%E5%88%B6%E8%A1%A8%E7%AC%A6/"/>
    
      <category term="regex_replace" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/regex-replace/"/>
    
  </entry>
  
  <entry>
    <title>python 拾遗</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/01/30/python-%E6%8B%BE%E9%81%97/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/01/30/python-拾遗/</id>
    <published>2018-01-30T12:09:42.000Z</published>
    <updated>2018-03-23T03:13:45.990Z</updated>
    
    <content type="html"><![CDATA[<h2 id="又是背景"><a href="#又是背景" class="headerlink" title="又是背景"></a>又是背景</h2><blockquote><p>python 作为工作大数据mapreduce脚本支撑语言用过一段时间了,通过这篇文章查缺补漏.<a id="more"></a></p></blockquote><h2 id="展开补漏"><a href="#展开补漏" class="headerlink" title="展开补漏"></a>展开补漏</h2><ol><li>字符串与其它类型数据连接问题</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> <span class="string">"hello"</span> + <span class="number">666</span></div></pre></td></tr></table></figure><blockquote><p>如上代码会报错如下:</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">TypeError: cannot concatenate <span class="string">'str'</span> <span class="keyword">and</span> <span class="string">'int'</span> objects</div></pre></td></tr></table></figure><p>解决方案如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> <span class="string">"hello"</span>+`<span class="number">666</span>`</div><div class="line">hello666</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> <span class="string">"hello"</span>+repr(<span class="number">666</span>)</div><div class="line">hello666</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> <span class="string">"hello"</span>+str(<span class="number">666</span>)</div><div class="line">hello666</div></pre></td></tr></table></figure><blockquote><p>如上的解决方案,第三种比较直观,也是常用的,反引号和repr则把结果字符串转换为合法的python表达式,现在python3中已经不再使用反引号了.<br>ps:str int long 属于数据基本类型转换,而repr属于函数</p></blockquote><ol><li>字符串与其它类型数据连接问题</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;又是背景&quot;&gt;&lt;a href=&quot;#又是背景&quot; class=&quot;headerlink&quot; title=&quot;又是背景&quot;&gt;&lt;/a&gt;又是背景&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;python 作为工作大数据mapreduce脚本支撑语言用过一段时间了,通过这篇文章查缺补漏.
    
    </summary>
    
    
      <category term="python" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>spark ListBuffer遍历</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/01/25/spark-ListBuffer%E9%81%8D%E5%8E%86/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/01/25/spark-ListBuffer遍历/</id>
    <published>2018-01-25T12:49:50.000Z</published>
    <updated>2018-03-23T03:13:28.194Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>hadoop streaming 使用中遇到的问题总结</title>
    <link href="https://www.tangyuxiaoyao.club/ITWO/2018/01/11/hadoop-streaming-%E4%BD%BF%E7%94%A8%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"/>
    <id>https://www.tangyuxiaoyao.club/ITWO/2018/01/11/hadoop-streaming-使用中遇到的问题总结/</id>
    <published>2018-01-11T01:45:13.000Z</published>
    <updated>2018-04-16T03:16:11.162Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><blockquote><p>hadoop生态圈中hadoop streaming占了很大的比重,因为它很好的融合了其他脚本语言作为map,reduce来处理业务逻辑,本文重点说的是使用Python或者shell遇到的问题.<br><a id="more"></a></p><h2 id="问题汇总"><a href="#问题汇总" class="headerlink" title="问题汇总"></a>问题汇总</h2><h3 id="last-tool-output-null"><a href="#last-tool-output-null" class="headerlink" title="last tool output null"></a>last tool output null</h3><p>报错信息如下</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">USER=lming_08  </div><div class="line">HADOOP_USER=<span class="keyword">null</span>  </div><div class="line">last tool output: |<span class="keyword">null</span>|  </div><div class="line">  </div><div class="line">java.io.IOException: Broken pipe</div></pre></td></tr></table></figure><blockquote><p>刚开始以为是 map 程序中对空行未处理导致的，后来发现是 map 中有对一个上传的文件进行读取，而程序对该配置文件,读取而且读取以后还会用正则去遍历模糊匹配关键词，在处理该文件时会执行 10 多亿次循环，导致超时了。<br>解决方案:yield block 的使用,少用正则匹配,可以利用切词然后join来完成这样的匹配,本来mapreduce就是为了空间换时间,利用集群多个节点去并行处理逻辑.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求背景&quot;&gt;&lt;a href=&quot;#需求背景&quot; class=&quot;headerlink&quot; title=&quot;需求背景&quot;&gt;&lt;/a&gt;需求背景&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;hadoop生态圈中hadoop streaming占了很大的比重,因为它很好的融合了其他脚本语言作为map,reduce来处理业务逻辑,本文重点说的是使用Python或者shell遇到的问题.&lt;br&gt;
    
    </summary>
    
    
      <category term="hadoop" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/hadoop/"/>
    
      <category term="streaming" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/streaming/"/>
    
      <category term="python" scheme="https://www.tangyuxiaoyao.club/ITWO/tags/python/"/>
    
  </entry>
  
</feed>
